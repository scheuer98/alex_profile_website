---
description: The Development of Rents in San Francisco # the title that will show up once someone gets to this page
draft: false
image: images/slider/sanfran.jpeg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work
keywords: ""
slug: homework2 # slug is the shorthand URL address... no spaces plz
title: "The Development of Rents in San Francisco"
author: "Group 1: Alex Scheuer, Dhruvi Mundra, Heng Jian Shun, Marta Wnek, Sharon Wenyu Xu, Xueying Liu"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
```

# Rents in San Francsisco 2000-2018

[Kate Pennington](https://www.katepennington.org/data) created a panel of historic Craigslist rents by scraping posts archived by the Wayback Machine. You can read more about her work here

[What impact does new housing have on rents, displacement, and gentrification in the surrounding neighborhood? Read our interview with economist Kate Pennington about her article, "Does Building New Housing Cause Displacement?:The Supply and Demand Effects of Construction in San Francisco."](https://matrix.berkeley.edu/research-article/kate-pennington-on-gentrification-and-displacement-in-san-francisco/)

In our case, we have a clean(ish) dataset with about 200K rows tht corresponf to Craigslist listings for renting properties in the greater SF area. The data dictionary is as follows

| variable    | class     | description           |
|-------------|-----------|-----------------------|
| post_id     | character | Unique ID             |
| date        | double    | date                  |
| year        | double    | year                  |
| nhood       | character | neighborhood          |
| city        | character | city                  |
| county      | character | county                |
| price       | double    | price in USD          |
| beds        | double    | n of beds             |
| baths       | double    | n of baths            |
| sqft        | double    | square feet of rental |
| room_in_apt | double    | room in apartment     |
| address     | character | address               |
| lat         | double    | latitude              |
| lon         | double    | longitude             |
| title       | character | title of listing      |
| descr       | character | description           |
| details     | character | additional details    |

The dataset was used in a recent [tidyTuesday](https://github.com/rfordatascience/tidytuesday) project.

```{r}
# download directly off tidytuesdaygithub repo

rent <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-05/rent.csv')

```

What are the variable types? Do they all correspond to what they really are? Which variables have most missing values?

The variable types are either character or double. On cursory glance, the variable types all tally. We sieved out all observations that had complete information to better check if the types correspond. From observation, the one variable type that seems not optimal is "date", which is input as a "dbl". This could (not should) be changed to the "date" type for heightened accuracy. The "chr" variables are all descriptors or names while the "dbl" variables are all numeric values; this checks out. The interesting thing to note is that the variable "address" is registered as "chr" type despite it being numeric; this is logical since the address is essentially a descriptor that serves as a categorical variable. The numeric aspect is not relevant for analysis. The top 5 variables that have the most missing values are "descr" (197542), "address" (196888), "lon" (196484), "lat" (193145) and "details" (192780).

```{r skim_data}
glimpse(rent)
skimr::skim(rent)

rent_nomissing <- rent %>% 
  drop_na()
head(rent_nomissing)
# Observe observations with complete information for better understanding of context

rent %>%
  summarise_all(~sum(is.na(.))) %>% 
  gather() %>% 
  arrange(desc(value))
# Check which variables have the most missing values

```

Make a plot that shows the top 20 cities in terms of % of classifieds between 2000-2018. You need to calculate the number of listings by city, and then convert that number to a %.

The final graph should look like this![](images/top_cities.png)

```{r top_cities}

# Creating dataset with top 20 cities
top_cities <- rent %>% 
  group_by(city) %>% 
  summarize(count_city = n()) %>% 
  arrange(desc(count_city)) %>% 
  mutate(frequency = count_city / sum(count_city), frequency = parse_number(scales::percent(frequency)))%>%
  top_n(20)

# Plotting top 20 cities by % listings
ggplot(data = top_cities, mapping = aes(x = frequency, y = fct_reorder(city, frequency))) +
  geom_col() +
  labs(title = "Top 20 cities in terms of % of classifieds from 2000-2018", x = "Frequency (%)", y = "City") +
  theme(axis.title = element_text(size = 12), axis.text = element_text(size = 10))
```

Make a plot that shows the evolution of median prices in San Francisco for 0, 1, 2, and 3 bedrooms listings. The final graph should look like this

![](images/sf_rentals.png)

```{r sf_median_prices}

# Created a table for the median price of flats with 3 or fewer bedrooms
median_price_beds <- rent %>% 
  filter(beds <= 3 & city == "san francisco") %>% 
  group_by(year, beds) %>% 
  summarise(median_price = median(price))
median_price_beds

# Plotted the median price of flats with 3 or fewer bedrooms from 2000-2018
ggplot(data = median_price_beds, mapping = aes(year, median_price, colour = factor(beds))) +
  facet_wrap(~ beds, ncol = 4) +
  geom_line() +
  scale_y_continuous(name = "Median price", breaks = seq(1000, 6000, 1000)) +
  labs(title = str_wrap("Evolution of median prices in San Francisco for different number of beds", 60), x = "Year", y = "Median Price") +
  theme(plot.title = element_text(size = 14, hjust = 0.5),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        strip.text.x = element_text(size = 5)) +
  theme(legend.key.size = unit(0.5, 'cm'), #change legend key size
        legend.key.height = unit(0.5, 'cm'), #change legend key height
        legend.key.width = unit(0.5, 'cm'), #change legend key width
        legend.title = element_text(size=10), #change legend title font size
        legend.text = element_text(size=8),
        legend.position = "bottom") #change legend text font size

```

Finally, make a plot that shows median rental prices for the top 12 cities in the Bay area. Your final graph should look like this

![](images/one_bed_bay_area.png)

```{r spirit_plot}

# We used the description provided rather than what was shown in the plots; i.e. we took data from all number of bedrooms rather than just 1-bedroom like shown in the plots.

# Extract city names for top 12 cities into new dataframe (took a subset from an earlier section)
top_12 <- top_cities$city[1:12]

# Summarised median prices per city per year for the top 12 cities
median_top12_cities <- rent %>% 
  filter(city %in% top_12) %>% 
  group_by(city, year) %>% 
  summarise(median_price = median(price))

# Plotted median prices per city per year for the top 12 cities from 2000-2018
ggplot(data = median_top12_cities, mapping = aes(year, median_price, colour = factor(city))) +
  facet_wrap(~ city, ncol = 4) +
  geom_line() +
  scale_y_continuous(name = "Median price", breaks = seq(1000, 6000, 1000)) +
  labs(title = "Median rental price for top 12 cities", x = "Year", y = "Median Price") +
  theme(plot.title = element_text(size = 14, hjust = 0.5),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.text.x = element_text(angle = 90),
        strip.text.x = element_text(size = 10)) +
  theme(legend.key.size = unit(0.5, 'cm'), #change legend key size
        legend.key.height = unit(0.5, 'cm'), #change legend key height
        legend.key.width = unit(0.5, 'cm'), #change legend key width
        legend.title = element_text(size=10), #change legend title font size
        legend.text = element_text(size=8), #change legend text font size
        legend.position = "right")
  

```

What can you infer from these plots? Don't just explain what's in the graph, but speculate or tell a short story (1-2 paragraphs max).

> TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.

From the first plot, we see that the median price of all flats with 3 or fewer bedrooms in San Francisco rose over the 15 years. It also seemed that the more bedrooms a flat had, the more severe the increase in price across years; 1-bed flats increased by about 100% of their original price after 15 years whereas 3-bed flats increased by 150%. We infer that a process of gentrification is at work here in the San Francisco region; wealthier individuals not only prefer, but also have the means to rent apartments with more luxurious amenities, e.g. more bedrooms. These individuals may engage in an auction process to out-bid their competitors to move into such apartments and thus prices rise more aggressively. One could also imagine that these apartments are within neighbourhoods with similar apartments; a process of gentrification then occurs since more wealthier individuals are attracted to these areas, thus fueling another cycle of competition and deepening the upward price spiral. While this cannot be seen from the plots, one could conduct spatial analysis/regression to test this narrative.

When the plots were faceted by cities, we see that the upward trend in prices still holds for all top 12 cities. However, the extent of overall increase in price within each city from 2000 to 2018 differs widely, with cities that experienced the highest price change suffering an almost 300% jump in price (Palo Alto) while other cities only saw a 100% increase in price. This suggests that some areas were more preferred than the others, perhaps due to better regional amenities maybe because they belonged to a more matured estate; the heightened demand for those areas caused an upward pressure on prices.

# Analysis of movies- IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)

```{r,load_movies, warning=FALSE, message=FALSE, eval=FALSE}

movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)

```

Besides the obvious variables of `title`, `genre`, `director`, `year`, and `duration`, the rest of the variables are as follows:

-   `gross` : The gross earnings in the US box office, not adjusted for inflation
-   `budget`: The movie's budget
-   `cast_facebook_likes`: the number of facebook likes cast memebrs received
-   `votes`: the number of people who voted for (or rated) the movie in IMDB
-   `reviews`: the number of reviews for that movie
-   `rating`: IMDB average rating

## Use your data import, inspection, and cleaning skills to answer the following:

-   Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?

There are no missing values; after dropping all observations with at least one missing value, the number of observations still remained the same. We observe that 53 movie tites had duplicates - 52 movie titles had one duplicate while the title "Home" had 2 duplicates.

```{r, cursory_check}

# Cursory glance
movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)
# First glance; data types for each variable seem appropriate

# Check for missing values
movies_nomissing <- movies %>% 
  drop_na()
# We see that after dropping all observations with at least one missing value, the number of observations remain the same.

# Check for missing values (alternate/clearer method)
movies %>%
  summarise_all(~sum(is.na(.))) %>% 
  gather() %>% 
  arrange(desc(value))
# Alternatively, We see that there are no missing values for any variables in the dataset.

# Identified all duplicates (please read the disclaimer in our comment below)
movies %>%
  group_by(title) %>% 
  filter(n()>1) %>%
  summarise(count(title))
# We observe that 53 movie tites had duplicates - 52 movie titles had one duplicate while the title "Home" had 2 duplicates. A disclaimer here: we note that for these duplicates, the "Voting" variable had different values. However, we do not use "Voting" in any further part of our exercise and hence decided that it would be more prudent to remove these duplicates since the values of other key variables for each of these movie titles were exactly the same.
```

```{r, cleaned_movies}

# Remove duplicates
cleaned_movies <- movies %>% 
  distinct(title, .keep_all = TRUE)
# Now, we clean the data and use this new dataframe henceforth.
```

Henceforth, the cleaned_movies dataframe will be used.

-   Produce a table with the count of movies by genre, ranked in descending order

```{r, movie_genres}

# Create a table with count of movies by genre, ranked in descending order
movie_by_genre <- cleaned_movies %>% 
  group_by(genre) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))

movie_by_genre
# View newly created table
```

-   Produce a table with the average gross earning and budget (`gross` and `budget`) by genre. Calculate a variable `return_on_budget` which shows how many \$ did a movie make at the box office for each \$ of its budget. Ranked genres by this `return_on_budget` in descending order

```{r, mean_grossandbudget}

# Summarise the gross, budget and return_on_budget variables
mean_grossandbudget <- cleaned_movies %>% 
  group_by(genre) %>% 
  summarise(mean_gross = mean(gross), mean_budget = mean(budget)) %>%
  mutate(return_on_budget = mean_gross / mean_budget) %>% 
  arrange(desc(return_on_budget))

# View newly created table
mean_grossandbudget

```

-   Produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. Don't just show the total gross amount, but also the mean, median, and standard deviation per director.

```{r, director_analysis}

# Summarise the total gross amount, the mean, median, and standard deviation for the top 15 directors by total gross amount
director_analysis <- cleaned_movies %>% 
  group_by(director) %>%
  summarise(total_gross = sum(gross),
            mean = mean(gross),
            median = median(gross),
            sd = sd(gross)) %>% 
  arrange(desc(total_gross)) %>% 
  top_n(15, total_gross)

# View newly created table
director_analysis

```

-   Finally, ratings. Produce a table that describes how ratings are distributed by genre. We don't want just the mean, but also, min, max, median, SD and some kind of a histogram or density graph that visually shows how ratings are distributed.

```{r, ratings_by_genre}

# Summarise the mean, min, max, median and SD of ratings for each genre of movies
ratings_by_genre <- cleaned_movies %>% 
  group_by(genre) %>% 
  summarise(mean = mean(rating),
            min = min(rating),
            max = max(rating),
            median = median(rating),
            sd = sd(rating))

ratings_by_genre
# View newly created table

ggplot(cleaned_movies, aes(rating)) +
  geom_density() +
  facet_wrap(~ genre, scales = "free") +
  labs(title = "Faceted density plots of movie ratings", x = "Ratings", y = "Density")
# Faceted density plots are not as effective or accurate because we earlier saw that there a select number of the genres (namely Sci-Fi, Family, Musical, Romance Western, Thriller) with especially few observations in them. The density function for those genres will thus not mean much.
# Notice also that the "Thriller" genre does not reflect any density plot because there was only one "Thriller" type movie within the entire dataset.

ggplot(cleaned_movies, aes(rating)) +
  geom_histogram() +
  facet_wrap(~ genre, scales = "free") +
  labs(title = "Faceted histogram plots of movie ratings", x = "Ratings", y = "Count")
# The histogram plot is able to reflect much of the distribution.

ggplot(cleaned_movies, aes(rating)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +  # Overlay with transparent density plot
    facet_wrap(~genre, scales = "free") +
    labs(title = "Faceted density plots overlaid on histograms of movie ratings", x = "Ratings", y = "Density")
# To view an overlay of a density plot on histogram (done just out of interest)
```

[***EDITOR'S NOTE:***]{.underline}

The faceted density plots are not as effective or accurate because we earlier saw that there is a select number of the genres (namely Sci-Fi, Family, Musical, Romance Western, Thriller) with especially few observations in them. The density function for those genres will thus not mean much. Also, we noticed that the "Thriller" genre does not reflect any density plot because there was only one "Thriller" type movie within the entire dataset. The histogram plots, on the other hand, are able to showcase the distributions.

## Use `ggplot` to answer the following

-   Examine the relationship between `gross` and `cast_facebook_likes`. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes?

```{r, gross_on_fblikes}

# Plot total gross on number of facebook likes
ggplot(cleaned_movies, aes(cast_facebook_likes, gross, colour = genre)) +
  geom_point() +
  geom_smooth(se = FALSE)+
  xlim(0, 200000) + # Limit the x-scale to make the interpretation simpler
  scale_x_continuous(labels = ~ format(.x, scientific = FALSE)) +
  scale_y_continuous(labels = ~ format(.x, scientific = FALSE)) +
  labs(title = "Scatterplot of total gross amount on number of facebook likes", x = "Number of Facebook Likes", y = "Gross") +
  theme(legend.key.size = unit(0.5, 'cm'), #change legend key size
        legend.key.height = unit(0.5, 'cm'), #change legend key height
        legend.key.width = unit(0.5, 'cm'), #change legend key width
        legend.title = element_text(size=10), #change legend title font size
        legend.text = element_text(size=8))

# As a sanity check, regress total gross on number of facebook likes
model <- lm(gross ~ cast_facebook_likes, data = cleaned_movies)
summary(model)
```

Although there is a positive correlation between the two variables, we believe that the facebook likes are not likely to be a good predictor of the money the movie will make at the box office based on the variability of outcomes as seen in the scatter plot. To confirm our intuition we did a basic regression of the likes on the gross revenues. We realize that there is a positive correlation and the predictor is significant, but based on the small r-squared value of around 0.05 we conclude that the predictive power of the facebook likes is not high.

-   Examine the relationship between `gross` and `budget`. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.

```{r, gross_on_budget}

# Plot scatterplot of total gross on budget
ggplot(cleaned_movies, aes(budget, gross, colour = genre)) +
  geom_point(aes(alpha = 0.1)) +
  geom_smooth(se = FALSE) +
  scale_x_continuous(labels = ~ format(.x, scientific = FALSE)) +
  labs(title = "Scatterplot of total gross amount on budget", x = "Budget", y = "Gross") +
  theme(legend.key.size = unit(0.5, 'cm'), #change legend key size
        legend.key.height = unit(0.5, 'cm'), #change legend key height
        legend.key.width = unit(0.5, 'cm'), #change legend key width
        legend.title = element_text(size=12), #change legend title font size
        legend.text = element_text(size=10))

# As a sanity check, regress total gross on budget
model <- lm(gross ~ budget, data = movies)
summary(model)
```

The scatter plot displays a clear trend of higher budgets for movies being linked with higher gross revenues on box office. A simple linear regression confirms this positive relation with an r-squared value of 0.41 and a highly significant budget coefficient.

-   Examine the relationship between `gross` and `rating`. Produce a scatterplot, faceted by `genre` and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?

```{r, gross_on_rating}

# Plot total gross on ratings
ggplot(cleaned_movies, aes(rating, gross, colour = factor(genre))) +
  geom_point(aes(alpha = 0.1)) +
  scale_x_continuous(labels = ~ format(.x, scientific = FALSE)) +
  ylim(0, 200000000) + # For clearer interpretation, we limit the y-scale
  facet_wrap(~ genre) +
  labs(title = "Scatterplot of total gross amount on rating", x = "Rating", y = "Gross") +
  theme(legend.key.size = unit(0.5, 'cm'), #change legend key size
        legend.key.height = unit(0.5, 'cm'), #change legend key height
        legend.key.width = unit(0.5, 'cm'), #change legend key width
        legend.title = element_text(size=12), #change legend title font size
        legend.text = element_text(size=10))

# As a sanity check, regress total gross on ratings
model <- lm(gross ~ rating, data = movies)
summary(model)
```

When plotting the rating against the gross revenues for the movies, we can clearly identify a positive relation. However, there is a strong spread for given ratings. This is confirmed by our simple least squared regression, yielding an r-squared value 0.07. When braking it down by genre we noticed for genres such as action there a lot of movies and whilst better ratings seem to be reflected in higher gross revenues, there still remain big discrepancies for given ratings (e.g. for a similar rating, the revenue may differ 10-fold). For other genres, such as documentaries, there seems to be no positive correlation between ratings and gross revenues. Though, this might also be down to a small sample size.

# Returns of financial stocks

> You may find useful the material on [finance data sources](https://mam2023.netlify.app/reference/finance_data/).

We will use the `tidyquant` package to download historical data of stock prices, calculate returns, and examine the distribution of returns.

We must first identify which stocks we want to download data for, and for this we must know their ticker symbol; Apple is known as AAPL, Microsoft as MSFT, McDonald's as MCD, etc. The file `nyse.csv` contains 508 stocks listed on the NYSE, their ticker `symbol`, `name`, the IPO (Initial Public Offering) year, and the sector and industry the company is in.

```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))
```

Based on this dataset, create a table and a bar plot that shows the number of companies per sector, in descending order

```{r companies_per_sector}

glimpse(nyse)

# Summarise count of companies per sector
condensed <- nyse %>%
  group_by(sector) %>% 
  summarize(count_company = n()) %>% 
  arrange(desc(count_company))

# View table
condensed

# Plot count of companies per sector
ggplot(data = condensed, mapping = aes(x = count_company, y = fct_reorder(sector, count_company))) +
  geom_col() +
  labs(title = "Number of companies per sector in the NYSE", x = "number of companies",y = "sector")


```

Next, let's choose some stocks and their ticker symbols and download some data. You **MUST** choose 6 different stocks from the ones listed below; You should, however, add `SPY` which is the SP500 ETF (Exchange Traded Fund).

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
# Notice the cache=TRUE argument inthe chunk options. Because getting data is time consuming, 
# cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks <- c("ACN","BLK","CPT","GM","HLT","LVS","SPY" ) %>%
  tq_get(get  = "stock.prices",
         from = "2011-01-01",
         to   = "2022-08-31") %>%
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame
```

Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

Create a table where you summarise monthly returns for each of the stocks and `SPY`; min, max, median, mean, SD.

```{r summarise_monthly_returns}

# Summarise min, max, median, mean, sd of chosen stocks
monthly_return_summary <- myStocks_returns_monthly %>%
  group_by(symbol) %>% 
  summarise(min = min(monthly_returns),
            max = max(monthly_returns),
            median = median(monthly_returns),
            mean = mean(monthly_returns),
            SD = sd(monthly_returns))

# View summary
monthly_return_summary
```

Plot a density plot, using `geom_density()`, for each of the stocks

```{r density_monthly_returns}

# Plot faceted monthly returns density plots for each stock
ggplot(myStocks_returns_monthly, aes(x = monthly_returns)) +
  geom_density() +
  facet_wrap(~ symbol) +
  labs(title = "Density Plots",
       subtitle = "Density Plots on Monthly Returns for each stock",
       x = "Monthly Returns",
       y = "Density")+
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```

What can you infer from this plot? Which stock is the riskiest? The least risky?

> TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.

The different stocks have various distributions among their monthly returns. The average monthly return of a stock tends to be close to zero in the long term (that is, the majority of the return is around 0, as shown in the graphs above). From our perspective, based on the table and the graphs above, LVS is the riskiest stock, and CPT is the least risky.

Finally, make a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use `ggrepel::geom_text_repel()` to label each stock

```{r risk_return_plot}

# Plot mean monthly returns on standard deviation (risk)
ggplot(monthly_return_summary, aes(x = SD, y = mean, label = symbol)) +
  geom_point() +
  ggrepel::geom_text_repel() +
  labs(title = "Return vs. Risk",
       subtitle = str_wrap("The expected monthly return (mean) of a stock and its corresponding risk (standard deviation)", 60),
       x = "Standard Deviation",
       y = "Average Monthly Return") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```

What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?

> TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.

Generally speaking, the riskier a stock is, the higher the expected return it would have. This can be observed in the group of stocks SPY, CPT, BLK, and HLT, as shown above. However, we see that GM and LVS with high risks do not have a higher expected return; this does not conform to our general understanding of high risk, high returns.

# On your own: Spotify

Spotify have an API, an Application Programming Interface. APIs are ways for computer programs to talk to each other. So while we use Spotify app to look up songs and artists, computers use the Spotify API to talk to the spotify server. There is an R package that allows R to talk to this API: [`spotifyr`](https://www.rcharlie.com/spotifyr/). One of your team members, need to sign up and get a [Spotify developer account](https://developer.spotify.com/dashboard/) and then you can download data about one's Spotify usage. A detailed article on how to go about it can be found here [Explore your activity on Spotify with R and *spotifyr*](https://towardsdatascience.com/explore-your-activity-on-spotify-with-r-and-spotifyr-how-to-analyze-and-visualize-your-stream-dee41cb63526)

If you do not want to use the API, you can download a sample of over 32K songs by having a look at <https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md>

```{r, download_spotify_data}

spotify_songs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')


```

The data dictionary can be found below

| **variable**             | **class** | **description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|-----------------|-----------------|---------------------------------------|
| track_id                 | character | Song unique ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| track_name               | character | Song Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| track_artist             | character | Song Artist                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| track_popularity         | double    | Song Popularity (0-100) where higher is better                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| track_album_id           | character | Album unique ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| track_album_name         | character | Song album name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| track_album_release_date | character | Date when album released                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| playlist_name            | character | Name of playlist                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| playlist_id              | character | Playlist ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| playlist_genre           | character | Playlist genre                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| playlist_subgenre        | character | Playlist subgenre                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| danceability             | double    | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.                                                                                                                                                                                                                                                                       |
| energy                   | double    | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.                                                                                                                          |
| key                      | double    | The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.                                                                                                                                                                                                                                                                                                                            |
| loudness                 | double    | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.                                                                                                                                                                                       |
| mode                     | double    | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.                                                                                                                                                                                                                                                                                                                                                    |
| speechiness              | double    | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |
| acousticness             | double    | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.                                                                                                                                                                                                                                                                                                                                                                                       |
| instrumentalness         | double    | Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.                                                                                                                 |
| liveness                 | double    | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.                                                                                                                                                                                                                                                                                            |
| valence                  | double    | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).                                                                                                                                                                                                                                                                  |
| tempo                    | double    | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.                                                                                                                                                                                                                                                                                                                         |
| duration_ms              | double    | Duration of song in milliseconds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |

In this dataset, there are only 6 types of `playlist_genre` , but we can still try to perform EDA on this dataset.

Produce a one-page summary describing this dataset. Here is a non-exhaustive list of questions:

1.  What is the distribution of songs' popularity (`track_popularity`). Does it look like a Normal distribution?

The songs' popularity score distribution resembles a normal distribution to a certain extent only; it looks somewhat symmetrical about the peak. However, there is one glaring exception; a significant number of songs are clustered on the lower end of the popularity score (score \<= 25), thus breaking the resemblance to a true normal distribution.

```{r, distributions}

# Density plot
ggplot(spotify_songs, aes(track_popularity)) +
  geom_density() +
  labs(x = "track popularity")

# Histogram plot
ggplot(spotify_songs, aes(track_popularity)) +
  geom_histogram() +
  labs(x = "track popularity")

# A clearer picture by super imposing density and histogram plots
ggplot(spotify_songs, aes(track_popularity)) +
  geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   colour="black", fill="white") +
  geom_density() +
  labs(x = "track popularity")

```

1.  There are 12 [audio features](https://developer.spotify.com/documentation/web-api/reference/object-model/#audio-features-object) for each track, including confidence measures like `acousticness`, `liveness`, `speechines`and `instrumentalness`, perceptual measures like `energy`, `loudness`, `danceability` and `valence` (positiveness), and descriptors like `duration`, `tempo`, `key`, and `mode`. How are they distributed? can you roughly guess which of these variables is closer to Normal just by looking at summary statistics?

The "danceability", "energy" and "duration_ms" variables have distributions that are closer to normal. The rest have extremely skewed or distorted distributions. The "mode" variable is a binary variable that takes on only values of 0 or 1.

```{r audio_feature_analysis}

# Create a long data table to represent each audio feature as a measurement score
audio_feature_list <- spotify_songs %>%
  gather(audio_feature, measurement, danceability:duration_ms, factor_key = TRUE)

# Create summary statistics and observe
summary_stats <- audio_feature_list %>%
  group_by(audio_feature) %>% 
  summarise(mean = mean(measurement),
            median = median(measurement),
            min = min(measurement),
            max = max(measurement),
            sd = sd(measurement))

# View summary stats
summary_stats

# Density plot
ggplot(audio_feature_list, aes(measurement)) +
  geom_density()+
  facet_wrap(~ audio_feature, scales = "free") +
  labs(title = "Audio Features' density plot analysis", x = "Measurement", y = "Density")

# Histogram plot
ggplot(audio_feature_list, aes(measurement)) +
  geom_histogram()+
  facet_wrap(~ audio_feature, scales = "free") +
  labs(title = "Audio Features' histogram analysis", x = "Measurement", y = "Count")

```

1.  How are `job_satisfaction` and `work_life_balance` distributed? Don't just report counts, but express categories as % of total

We are unsure if this question above belongs in this homework set since the variables "job_satisfaction" and "work_life_balance" are not found in the Spotify songs dataset.

1.  Is there any relationship between `valence` and `track_popularity`? `danceability` and `track_popularity` ?

The scatterplots of both track popularity on valence and track popularity on danceability reflect no clear relatioship; no clear line can be plotted through the points. An interpretation could be that every person's music taste is really diverse and both valence and danceability are no significant indicators of whether a track will be a hit or miss amongst the crowd.

```{r, popularity_on_valence}

# Produce scatterplot of track popularity on valence
ggplot(spotify_songs, aes(valence, track_popularity)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  labs(title = "Scatterplot of Track Popularity on Valence", x = "Valence", y = "Track Popularity")

# As a sanity check, regress popularity on valence
model <- lm(track_popularity ~ valence, data = spotify_songs)
summary(model)
```

```{r, popularity_on_danceability}

ggplot(spotify_songs, aes(danceability, track_popularity)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  labs(title = "Scatterplot of Track Popularity on Danceability", x = "Danceability", y = "Track Popularity")
# Produce scatterplot of track popularity on danceability

# As a sanity check, regress popularity on valence
model <- lm(track_popularity ~ danceability, data = spotify_songs)
summary(model)
```

1.  `mode` indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. Do songs written on a major scale have higher `danceability` compared to those in minor scale? What about `track_popularity`?

From the scatterplot, there is no clear relationship between danceability and mode. We should expect to see a clustering in opposite diagonals of the scatterplot if there were a relationship between both variables; however, that's not what we see for both the points along the mode = 1 line and the points along the mode = 0 line. Similarly, we do not see a clear relationship between track popularity and mode. Even going one step further to observe the relationship within each genre does not yield any change.

```{r, modality_on_danceability}

# Create scatterplot of mode on danceability
ggplot(spotify_songs, aes(danceability, mode)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  labs(title = "Scatterplot of Mode on Danceability", x = "Danceability", y = "Mode")

# Create scatterplot of mode on danceability
ggplot(spotify_songs, aes(danceability, mode)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  facet_wrap(~ playlist_genre) +
  labs(title = "Faceted scatterplots of Mode on Danceability", x = "Danceability", y = "Mode")
```

```{r, modality_on_popularity}
# Create scatterplot of mode on track popularity according to genre
ggplot(spotify_songs, aes(track_popularity, mode)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  labs(title = "Scatterplot of Mode on Track Popularity", x = "Track Popularity", y = "Mode")

# Create scatterplot of track popularity on danceability for each of the 6 genres
ggplot(spotify_songs, aes(track_popularity, mode)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  facet_wrap(~ playlist_genre) +
  labs(title = "Faceted scatterplots of Mode on Track Popularity", x = "Track Popularity", y = "Mode")

```

Narrative:

Overall, the songs' popularity score distribution resemble a normal distribution to a certain extent only; it looks somewhat symmetrical about the peak. However, there is one glaring exception; a significant number of songs are clustered on the lower end of the popularity score (score \<= 25), thus breaking the resemblance to a true normal distribution. In terms of the different audio features, the "danceability", "energy" and "duration_ms" variables have distributions that are closer to normal. The rest have extremely skewed or distorted distributions. The "mode" variable is a binary variable that takes on only values of 0 or 1.

Now, we move on to drawing relationships between key variables. The scatterplots of both track popularity on valence and track popularity on danceability reflect no clear relatioship; no clear line can be plotted through the points. An interpretation could be that every person's music taste is really diverse and both valence and danceability are no significant indicators of whether a track will be a hit or miss amongst the crowd. From the scatterplots, there is also no clear relationship between danceability and mode. We should expect to see a clustering in opposite diagonals of the scatterplot if there were a relationship between both variables; however, that's not what we see for both the points along the mode = 1 line and the points along the mode = 0 line. Similarly, we do not see a clear relationship between track popularity and mode. Even going one step further to observe the relationship within each genre does not yield any change.

# Challenge 1: Replicating a chart

The purpose of this exercise is to reproduce a plot using your `dplyr` and `ggplot2` skills. It builds on exercise 1, the San Francisco rentals data.

You have to create a graph that calculates the cumulative % change for 0-, 1-1, and 2-bed flats between 2000 and 2018 for the top twelve cities in Bay Area, by number of ads that appeared in Craigslist. Your final graph should look like this

![](images/challenge1.png)

```{r percent_change_top_12}

# Summarise popularity of cities in terms of percent of ads that appeared
# Find the top 12 cities (We reused code from the section above since they are linked)
top_12_cities <- rent %>% 
  group_by(city) %>% 
  summarize(count_city = n()) %>% 
  arrange(desc(count_city)) %>% 
  mutate(frequency = count_city / sum(count_city)) %>% 
  top_n(12, frequency)

# View top 12 cities
top_12_cities

# Based on the cumulative_percent_change code/formula that you sent us via Slack, we focused on the cumulative median price change
cumulative_percent_change <- rent %>% 
  filter(city %in% top_12_cities$city & beds <= 2) %>% 
  group_by(city, beds, year) %>%
  summarise(median_price = median(price)) %>%
  ungroup() %>% 
  mutate(pct_change = (median_price/lag(median_price))) %>% 
  mutate(pct_change = ifelse(is.na(pct_change), 1, pct_change)) %>% 
  mutate(percent_change = cumprod(pct_change), percent_change = parse_number(scales::percent(percent_change)))

# Plot cumulative percent median price change across time for each of the unique bed-city combination  
ggplot(data = cumulative_percent_change, mapping = aes(year, percent_change, colour = factor(city))) +
    facet_grid(vars(beds), vars(city)) +
    geom_line() +
    labs(title = "Cumulative Percentage Change for flats", x = "Year", y = "Cumulative Percent Change (%)", ) +
    theme(axis.title = element_text(size = 14),
          axis.text = element_text(size = 8),
          axis.text.x = element_text(angle = 90),
          strip.text = element_text(size = 5),
          strip.background = element_rect(fill="lightblue", colour="black", size=1),
          legend.key.size = unit(0.5, 'cm'), #change legend key size
          legend.key.height = unit(0.5, 'cm'), #change legend key height
          legend.key.width = unit(0.5, 'cm'), #change legend key width
          legend.title = element_text(size=10), #change legend title font size
          legend.text = element_text(size=8),
          legend.position = "bottom") +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "Cities", breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = "Beds", breaks = NULL, labels = NULL))
  
```

# Challenge 2: 2016 California Contributors plots

As discussed in class, I would like you to reproduce the plot that shows the top ten cities in highest amounts raised in political contributions in California during the 2016 US Presidential election.

```{r challenge2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "challenge2.png"), error = FALSE)
```

To get this plot, you must join two dataframes; the one you have with all contributions, and data that can translate zipcodes to cities. You can find a file with all US zipcodes, e.g., here <http://www.uszipcodelist.com/download.html>.

The easiest way would be to create two plots and then place one next to each other. For this, you will need the `patchwork` package. <https://cran.r-project.org/web/packages/patchwork/index.html>

While this is ok, what if one asked you to create the same plot for the top 10 candidates and not just the top two? The most challenging part is how to reorder within categories, and for this you will find Julia Silge's post on [REORDERING AND FACETTING FOR GGPLOT2](https://juliasilge.com/blog/reorder-within/) useful.

```{r, load_CA_data, warnings= FALSE, message=FALSE}
# Make sure you use vroom() as it is significantly faster than read.csv()
# Load datasets
CA_contributors_2016 <- vroom::vroom(here::here("data","CA_contributors_2016.csv"))

zipcodes <- readr::read_csv(here::here("data", "zip_code_database.csv"))

# Glimpse datasets
glimpse(CA_contributors_2016)
glimpse(zipcodes)

# Join datasets (note that zip types are different across both datasets)
# Therefore we change them to be the same so that the datasets can be joined
CA_contributors_2016$zip<-as.character(CA_contributors_2016$zip)
df <- left_join(CA_contributors_2016, zipcodes, "zip")

# Summarise total contributions raised by the top 10 candidates across every city
top_10_cand_overall <- df %>% 
  group_by(cand_nm) %>%
  summarize(total = sum(contb_receipt_amt)) %>%
  arrange(desc(total)) %>% 
  top_n(10)

# Create long form data to incude only the top 10 candidates
long_form_top10_cand_all_cities <- df %>% 
  filter(cand_nm %in% top_10_cand_overall$cand_nm) %>% 
  group_by(cand_nm, primary_city) %>%
  summarise(total_contrib = sum(contb_receipt_amt))

# From previous dataset, create a table for the top 10 candidates across the top 10 cities
# Incorporate the use of reorder_within to reorder within a group
top_10_cities_per_cand <- long_form_top10_cand_all_cities %>% 
  group_by(cand_nm) %>% 
  top_n(10, total_contrib) %>% 
  ungroup() %>%
  mutate(cand_nm = as.factor(cand_nm),
         primary_city = tidytext::reorder_within(primary_city, total_contrib, cand_nm)) 

# Plot faceted plots for each of the top 10 candidates across their respective top 10 cities
ggplot(top_10_cities_per_cand , aes(total_contrib, primary_city))+
  geom_col(aes(fill = cand_nm), show.legend = FALSE) +
  facet_wrap(~ cand_nm, ncol = 2, scales = "free") +
  tidytext::scale_y_reordered() +
  labs(title = "Comparisonn of contribution amounts raised", subtitle = "In which cities did the top 10 candidates raise the most money?", x = "Amount raised", y = "City") +
  theme(axis.title = element_text(size = 14),
          axis.text = element_text(size = 8),
          strip.text = element_text(size = 5),
          strip.background = element_rect(fill="lightblue", colour="black", size=1))+
  scale_x_continuous(labels = ~ format(.x, scientific = FALSE),
                     sec.axis = sec_axis(~ . , name = "Candidates", breaks = NULL, labels = NULL))

```

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.

# Details

-   Who did you collaborate with: [Alex Scheuer, Dhruvi Mundra, Heng Jian Shun, Marta Wnek, Sharon Wenyu Xu, Xueying Liu]{.underline}

-   Approximately how much time did you spend on this problem set: [20 hours]{.underline}

-   What, if anything, gave you the most trouble: R-interface keeps giving problems. [Files keep running into errors when trying to knit them into html.]{.underline}

**Please seek out help when you need it,** and remember the [15-minute rule](https://mam2022.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

[Yes we are able to.]{.underline}

# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed.

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output).

Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.

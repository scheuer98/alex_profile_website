---
description: The Development of Rents in San Francisco # the title that will show up once someone gets to this page
draft: false
image: images/slider/sanfran.jpeg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work
keywords: ""
slug: homework2 # slug is the shorthand URL address... no spaces plz
title: "The Development of Rents in San Francisco"
author: "Group 1: Alex Scheuer, Dhruvi Mundra, Heng Jian Shun, Marta Wnek, Sharon Wenyu Xu, Xueying Liu"
date: "2022-09-20"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---



<div id="rents-in-san-francsisco-2000-2018" class="section level1">
<h1>Rents in San Francsisco 2000-2018</h1>
<p><a href="https://www.katepennington.org/data">Kate Pennington</a> created a panel of historic Craigslist rents by scraping posts archived by the Wayback Machine. You can read more about her work here</p>
<p><a href="https://matrix.berkeley.edu/research-article/kate-pennington-on-gentrification-and-displacement-in-san-francisco/">What impact does new housing have on rents, displacement, and gentrification in the surrounding neighborhood? Read our interview with economist Kate Pennington about her article, “Does Building New Housing Cause Displacement?:The Supply and Demand Effects of Construction in San Francisco.”</a></p>
<p>In our case, we have a clean(ish) dataset with about 200K rows tht corresponf to Craigslist listings for renting properties in the greater SF area. The data dictionary is as follows</p>
<table>
<thead>
<tr class="header">
<th>variable</th>
<th>class</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>post_id</td>
<td>character</td>
<td>Unique ID</td>
</tr>
<tr class="even">
<td>date</td>
<td>double</td>
<td>date</td>
</tr>
<tr class="odd">
<td>year</td>
<td>double</td>
<td>year</td>
</tr>
<tr class="even">
<td>nhood</td>
<td>character</td>
<td>neighborhood</td>
</tr>
<tr class="odd">
<td>city</td>
<td>character</td>
<td>city</td>
</tr>
<tr class="even">
<td>county</td>
<td>character</td>
<td>county</td>
</tr>
<tr class="odd">
<td>price</td>
<td>double</td>
<td>price in USD</td>
</tr>
<tr class="even">
<td>beds</td>
<td>double</td>
<td>n of beds</td>
</tr>
<tr class="odd">
<td>baths</td>
<td>double</td>
<td>n of baths</td>
</tr>
<tr class="even">
<td>sqft</td>
<td>double</td>
<td>square feet of rental</td>
</tr>
<tr class="odd">
<td>room_in_apt</td>
<td>double</td>
<td>room in apartment</td>
</tr>
<tr class="even">
<td>address</td>
<td>character</td>
<td>address</td>
</tr>
<tr class="odd">
<td>lat</td>
<td>double</td>
<td>latitude</td>
</tr>
<tr class="even">
<td>lon</td>
<td>double</td>
<td>longitude</td>
</tr>
<tr class="odd">
<td>title</td>
<td>character</td>
<td>title of listing</td>
</tr>
<tr class="even">
<td>descr</td>
<td>character</td>
<td>description</td>
</tr>
<tr class="odd">
<td>details</td>
<td>character</td>
<td>additional details</td>
</tr>
</tbody>
</table>
<p>The dataset was used in a recent <a href="https://github.com/rfordatascience/tidytuesday">tidyTuesday</a> project.</p>
<pre class="r"><code># download directly off tidytuesdaygithub repo

rent &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-05/rent.csv&#39;)</code></pre>
<p>What are the variable types? Do they all correspond to what they really are? Which variables have most missing values?</p>
<p>The variable types are either character or double. On cursory glance, the variable types all tally. We sieved out all observations that had complete information to better check if the types correspond. From observation, the one variable type that seems not optimal is “date”, which is input as a “dbl”. This could (not should) be changed to the “date” type for heightened accuracy. The “chr” variables are all descriptors or names while the “dbl” variables are all numeric values; this checks out. The interesting thing to note is that the variable “address” is registered as “chr” type despite it being numeric; this is logical since the address is essentially a descriptor that serves as a categorical variable. The numeric aspect is not relevant for analysis. The top 5 variables that have the most missing values are “descr” (197542), “address” (196888), “lon” (196484), “lat” (193145) and “details” (192780).</p>
<pre class="r"><code>glimpse(rent)</code></pre>
<pre><code>## Rows: 200,796
## Columns: 17
## $ post_id     &lt;chr&gt; &quot;pre2013_134138&quot;, &quot;pre2013_135669&quot;, &quot;pre2013_127127&quot;, &quot;pre…
## $ date        &lt;dbl&gt; 20050111, 20050126, 20041017, 20120601, 20041021, 20060411…
## $ year        &lt;dbl&gt; 2005, 2005, 2004, 2012, 2004, 2006, 2007, 2017, 2009, 2006…
## $ nhood       &lt;chr&gt; &quot;alameda&quot;, &quot;alameda&quot;, &quot;alameda&quot;, &quot;alameda&quot;, &quot;alameda&quot;, &quot;al…
## $ city        &lt;chr&gt; &quot;alameda&quot;, &quot;alameda&quot;, &quot;alameda&quot;, &quot;alameda&quot;, &quot;alameda&quot;, &quot;al…
## $ county      &lt;chr&gt; &quot;alameda&quot;, &quot;alameda&quot;, &quot;alameda&quot;, &quot;alameda&quot;, &quot;alameda&quot;, &quot;al…
## $ price       &lt;dbl&gt; 1250, 1295, 1100, 1425, 890, 825, 1500, 2925, 450, 1395, 1…
## $ beds        &lt;dbl&gt; 2, 2, 2, 1, 1, 1, 1, 3, NA, 2, 2, 5, 4, 0, 4, 1, 3, 3, 1, …
## $ baths       &lt;dbl&gt; 2, NA, NA, NA, NA, NA, 1, NA, 1, NA, NA, NA, 3, NA, NA, NA…
## $ sqft        &lt;dbl&gt; NA, NA, NA, 735, NA, NA, NA, NA, NA, NA, NA, 2581, 1756, N…
## $ room_in_apt &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ address     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
## $ lat         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 37.5, NA, …
## $ lon         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
## $ title       &lt;chr&gt; &quot;$1250 / 2br - 2BR/2BA   1145 ALAMEDA DE LAS PULGAS&quot;, &quot;$12…
## $ descr       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…
## $ details     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;&lt;p class=…</code></pre>
<pre class="r"><code>skimr::skim(rent)</code></pre>
<table>
<caption>(#tab:skim_data)Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">rent</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">200796</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">17</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">8</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">9</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<colgroup>
<col width="18%" />
<col width="13%" />
<col width="18%" />
<col width="5%" />
<col width="8%" />
<col width="8%" />
<col width="12%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">post_id</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">9</td>
<td align="right">14</td>
<td align="right">0</td>
<td align="right">200796</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">nhood</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">4</td>
<td align="right">43</td>
<td align="right">0</td>
<td align="right">167</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">city</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">5</td>
<td align="right">19</td>
<td align="right">0</td>
<td align="right">104</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">county</td>
<td align="right">1394</td>
<td align="right">0.99</td>
<td align="right">4</td>
<td align="right">13</td>
<td align="right">0</td>
<td align="right">10</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">address</td>
<td align="right">196888</td>
<td align="right">0.02</td>
<td align="right">1</td>
<td align="right">38</td>
<td align="right">0</td>
<td align="right">2869</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">title</td>
<td align="right">2517</td>
<td align="right">0.99</td>
<td align="right">2</td>
<td align="right">298</td>
<td align="right">0</td>
<td align="right">184961</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">descr</td>
<td align="right">197542</td>
<td align="right">0.02</td>
<td align="right">13</td>
<td align="right">16975</td>
<td align="right">0</td>
<td align="right">3025</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">details</td>
<td align="right">192780</td>
<td align="right">0.04</td>
<td align="right">4</td>
<td align="right">595</td>
<td align="right">0</td>
<td align="right">7667</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<colgroup>
<col width="12%" />
<col width="8%" />
<col width="12%" />
<col width="8%" />
<col width="7%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="5%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">date</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2.01e+07</td>
<td align="right">44694.07</td>
<td align="right">2.00e+07</td>
<td align="right">2.01e+07</td>
<td align="right">2.01e+07</td>
<td align="right">2.01e+07</td>
<td align="right">2.02e+07</td>
<td align="left">▁▇▁▆▃</td>
</tr>
<tr class="even">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2.01e+03</td>
<td align="right">4.48</td>
<td align="right">2.00e+03</td>
<td align="right">2.00e+03</td>
<td align="right">2.01e+03</td>
<td align="right">2.01e+03</td>
<td align="right">2.02e+03</td>
<td align="left">▁▇▁▆▃</td>
</tr>
<tr class="odd">
<td align="left">price</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2.14e+03</td>
<td align="right">1427.75</td>
<td align="right">2.20e+02</td>
<td align="right">1.30e+03</td>
<td align="right">1.80e+03</td>
<td align="right">2.50e+03</td>
<td align="right">4.00e+04</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">beds</td>
<td align="right">6608</td>
<td align="right">0.97</td>
<td align="right">1.89e+00</td>
<td align="right">1.08</td>
<td align="right">0.00e+00</td>
<td align="right">1.00e+00</td>
<td align="right">2.00e+00</td>
<td align="right">3.00e+00</td>
<td align="right">1.20e+01</td>
<td align="left">▇▂▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">baths</td>
<td align="right">158121</td>
<td align="right">0.21</td>
<td align="right">1.68e+00</td>
<td align="right">0.69</td>
<td align="right">1.00e+00</td>
<td align="right">1.00e+00</td>
<td align="right">2.00e+00</td>
<td align="right">2.00e+00</td>
<td align="right">8.00e+00</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">sqft</td>
<td align="right">136117</td>
<td align="right">0.32</td>
<td align="right">1.20e+03</td>
<td align="right">5000.22</td>
<td align="right">8.00e+01</td>
<td align="right">7.50e+02</td>
<td align="right">1.00e+03</td>
<td align="right">1.36e+03</td>
<td align="right">9.00e+05</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">room_in_apt</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">0.00e+00</td>
<td align="right">0.04</td>
<td align="right">0.00e+00</td>
<td align="right">0.00e+00</td>
<td align="right">0.00e+00</td>
<td align="right">0.00e+00</td>
<td align="right">1.00e+00</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">lat</td>
<td align="right">193145</td>
<td align="right">0.04</td>
<td align="right">3.77e+01</td>
<td align="right">0.35</td>
<td align="right">3.36e+01</td>
<td align="right">3.74e+01</td>
<td align="right">3.78e+01</td>
<td align="right">3.78e+01</td>
<td align="right">4.04e+01</td>
<td align="left">▁▁▅▇▁</td>
</tr>
<tr class="odd">
<td align="left">lon</td>
<td align="right">196484</td>
<td align="right">0.02</td>
<td align="right">-1.22e+02</td>
<td align="right">0.78</td>
<td align="right">-1.23e+02</td>
<td align="right">-1.22e+02</td>
<td align="right">-1.22e+02</td>
<td align="right">-1.22e+02</td>
<td align="right">-7.42e+01</td>
<td align="left">▇▁▁▁▁</td>
</tr>
</tbody>
</table>
<pre class="r"><code>rent_nomissing &lt;- rent %&gt;% 
  drop_na()
head(rent_nomissing)</code></pre>
<pre><code>## # A tibble: 6 × 17
##   post_id        date  year nhood   city  county price  beds baths  sqft room_…¹
##   &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 4710888130 20141012  2014 alameda alam… alame…  2250     2     1  1080       0
## 2 4988581576 20150421  2015 alameda alam… alame…  2650     2     1   950       0
## 3 4988561264 20150421  2015 alameda alam… alame…  1950     2     1   800       0
## 4 4855533017 20150120  2015 alameda alam… alame…  2650     2     1   950       0
## 5 4631188738 20140824  2014 alameda alam… alame…  3295     4     1  1716       0
## 6 4794051100 20141209  2014 alameda alam… alame…  1860     1     1   705       0
## # … with 6 more variables: address &lt;chr&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, title &lt;chr&gt;,
## #   descr &lt;chr&gt;, details &lt;chr&gt;, and abbreviated variable name ¹​room_in_apt</code></pre>
<pre class="r"><code># Observe observations with complete information for better understanding of context

rent %&gt;%
  summarise_all(~sum(is.na(.))) %&gt;% 
  gather() %&gt;% 
  arrange(desc(value))</code></pre>
<pre><code>## # A tibble: 17 × 2
##    key          value
##    &lt;chr&gt;        &lt;int&gt;
##  1 descr       197542
##  2 address     196888
##  3 lon         196484
##  4 lat         193145
##  5 details     192780
##  6 baths       158121
##  7 sqft        136117
##  8 beds          6608
##  9 title         2517
## 10 county        1394
## 11 post_id          0
## 12 date             0
## 13 year             0
## 14 nhood            0
## 15 city             0
## 16 price            0
## 17 room_in_apt      0</code></pre>
<pre class="r"><code># Check which variables have the most missing values</code></pre>
<p>Make a plot that shows the top 20 cities in terms of % of classifieds between 2000-2018. You need to calculate the number of listings by city, and then convert that number to a %.</p>
<p>The final graph should look like this<img src="images/top_cities.png" /></p>
<pre class="r"><code># Creating dataset with top 20 cities
top_cities &lt;- rent %&gt;% 
  group_by(city) %&gt;% 
  summarize(count_city = n()) %&gt;% 
  arrange(desc(count_city)) %&gt;% 
  mutate(frequency = count_city / sum(count_city), frequency = parse_number(scales::percent(frequency)))%&gt;%
  top_n(20)

# Plotting top 20 cities by % listings
ggplot(data = top_cities, mapping = aes(x = frequency, y = fct_reorder(city, frequency))) +
  geom_col() +
  labs(title = &quot;Top 20 cities in terms of % of classifieds from 2000-2018&quot;, x = &quot;Frequency (%)&quot;, y = &quot;City&quot;) +
  theme(axis.title = element_text(size = 12), axis.text = element_text(size = 10))</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/top_cities-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>Make a plot that shows the evolution of median prices in San Francisco for 0, 1, 2, and 3 bedrooms listings. The final graph should look like this</p>
<p><img src="images/sf_rentals.png" /></p>
<pre class="r"><code># Created a table for the median price of flats with 3 or fewer bedrooms
median_price_beds &lt;- rent %&gt;% 
  filter(beds &lt;= 3 &amp; city == &quot;san francisco&quot;) %&gt;% 
  group_by(year, beds) %&gt;% 
  summarise(median_price = median(price))
median_price_beds</code></pre>
<pre><code>## # A tibble: 75 × 3
## # Groups:   year [19]
##     year  beds median_price
##    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;
##  1  2000     0        1100 
##  2  2000     1        2175 
##  3  2000     2        2098.
##  4  2001     0        1250 
##  5  2001     1        1725 
##  6  2001     2        2250 
##  7  2001     3        2995 
##  8  2002     0        1125 
##  9  2002     1        1532.
## 10  2002     2        1972.
## # … with 65 more rows</code></pre>
<pre class="r"><code># Plotted the median price of flats with 3 or fewer bedrooms from 2000-2018
ggplot(data = median_price_beds, mapping = aes(year, median_price, colour = factor(beds))) +
  facet_wrap(~ beds, ncol = 4) +
  geom_line() +
  scale_y_continuous(name = &quot;Median price&quot;, breaks = seq(1000, 6000, 1000)) +
  labs(title = str_wrap(&quot;Evolution of median prices in San Francisco for different number of beds&quot;, 60), x = &quot;Year&quot;, y = &quot;Median Price&quot;) +
  theme(plot.title = element_text(size = 14, hjust = 0.5),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        strip.text.x = element_text(size = 5)) +
  theme(legend.key.size = unit(0.5, &#39;cm&#39;), #change legend key size
        legend.key.height = unit(0.5, &#39;cm&#39;), #change legend key height
        legend.key.width = unit(0.5, &#39;cm&#39;), #change legend key width
        legend.title = element_text(size=10), #change legend title font size
        legend.text = element_text(size=8),
        legend.position = &quot;bottom&quot;) #change legend text font size</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/sf_median_prices-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>Finally, make a plot that shows median rental prices for the top 12 cities in the Bay area. Your final graph should look like this</p>
<p><img src="images/one_bed_bay_area.png" /></p>
<pre class="r"><code># We used the description provided rather than what was shown in the plots; i.e. we took data from all number of bedrooms rather than just 1-bedroom like shown in the plots.

# Extract city names for top 12 cities into new dataframe (took a subset from an earlier section)
top_12 &lt;- top_cities$city[1:12]

# Summarised median prices per city per year for the top 12 cities
median_top12_cities &lt;- rent %&gt;% 
  filter(city %in% top_12) %&gt;% 
  group_by(city, year) %&gt;% 
  summarise(median_price = median(price))

# Plotted median prices per city per year for the top 12 cities from 2000-2018
ggplot(data = median_top12_cities, mapping = aes(year, median_price, colour = factor(city))) +
  facet_wrap(~ city, ncol = 4) +
  geom_line() +
  scale_y_continuous(name = &quot;Median price&quot;, breaks = seq(1000, 6000, 1000)) +
  labs(title = &quot;Median rental price for top 12 cities&quot;, x = &quot;Year&quot;, y = &quot;Median Price&quot;) +
  theme(plot.title = element_text(size = 14, hjust = 0.5),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.text.x = element_text(angle = 90),
        strip.text.x = element_text(size = 10)) +
  theme(legend.key.size = unit(0.5, &#39;cm&#39;), #change legend key size
        legend.key.height = unit(0.5, &#39;cm&#39;), #change legend key height
        legend.key.width = unit(0.5, &#39;cm&#39;), #change legend key width
        legend.title = element_text(size=10), #change legend title font size
        legend.text = element_text(size=8), #change legend text font size
        legend.position = &quot;right&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/spirit_plot-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>What can you infer from these plots? Don’t just explain what’s in the graph, but speculate or tell a short story (1-2 paragraphs max).</p>
<blockquote>
<p>TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.</p>
</blockquote>
<p>From the first plot, we see that the median price of all flats with 3 or fewer bedrooms in San Francisco rose over the 15 years. It also seemed that the more bedrooms a flat had, the more severe the increase in price across years; 1-bed flats increased by about 100% of their original price after 15 years whereas 3-bed flats increased by 150%. We infer that a process of gentrification is at work here in the San Francisco region; wealthier individuals not only prefer, but also have the means to rent apartments with more luxurious amenities, e.g. more bedrooms. These individuals may engage in an auction process to out-bid their competitors to move into such apartments and thus prices rise more aggressively. One could also imagine that these apartments are within neighbourhoods with similar apartments; a process of gentrification then occurs since more wealthier individuals are attracted to these areas, thus fueling another cycle of competition and deepening the upward price spiral. While this cannot be seen from the plots, one could conduct spatial analysis/regression to test this narrative.</p>
<p>When the plots were faceted by cities, we see that the upward trend in prices still holds for all top 12 cities. However, the extent of overall increase in price within each city from 2000 to 2018 differs widely, with cities that experienced the highest price change suffering an almost 300% jump in price (Palo Alto) while other cities only saw a 100% increase in price. This suggests that some areas were more preferred than the others, perhaps due to better regional amenities maybe because they belonged to a more matured estate; the heightened demand for those areas caused an upward pressure on prices.</p>
</div>
<div id="analysis-of-movies--imdb-dataset" class="section level1">
<h1>Analysis of movies- IMDB dataset</h1>
<p>We will look at a subset sample of movies, taken from the <a href="https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset">Kaggle IMDB 5000 movie dataset</a></p>
<pre class="r"><code>movies &lt;- read_csv(here::here(&quot;data&quot;, &quot;movies.csv&quot;))
glimpse(movies)</code></pre>
<p>Besides the obvious variables of <code>title</code>, <code>genre</code>, <code>director</code>, <code>year</code>, and <code>duration</code>, the rest of the variables are as follows:</p>
<ul>
<li><code>gross</code> : The gross earnings in the US box office, not adjusted for inflation</li>
<li><code>budget</code>: The movie’s budget</li>
<li><code>cast_facebook_likes</code>: the number of facebook likes cast memebrs received</li>
<li><code>votes</code>: the number of people who voted for (or rated) the movie in IMDB</li>
<li><code>reviews</code>: the number of reviews for that movie</li>
<li><code>rating</code>: IMDB average rating</li>
</ul>
<div id="use-your-data-import-inspection-and-cleaning-skills-to-answer-the-following" class="section level2">
<h2>Use your data import, inspection, and cleaning skills to answer the following:</h2>
<ul>
<li>Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?</li>
</ul>
<p>There are no missing values; after dropping all observations with at least one missing value, the number of observations still remained the same. We observe that 53 movie tites had duplicates - 52 movie titles had one duplicate while the title “Home” had 2 duplicates.</p>
<pre class="r"><code># Cursory glance
movies &lt;- read_csv(here::here(&quot;data&quot;, &quot;movies.csv&quot;))
glimpse(movies)</code></pre>
<pre><code>## Rows: 2,961
## Columns: 11
## $ title               &lt;chr&gt; &quot;Avatar&quot;, &quot;Titanic&quot;, &quot;Jurassic World&quot;, &quot;The Avenge…
## $ genre               &lt;chr&gt; &quot;Action&quot;, &quot;Drama&quot;, &quot;Action&quot;, &quot;Action&quot;, &quot;Action&quot;, &quot;…
## $ director            &lt;chr&gt; &quot;James Cameron&quot;, &quot;James Cameron&quot;, &quot;Colin Trevorrow…
## $ year                &lt;dbl&gt; 2009, 1997, 2015, 2012, 2008, 1999, 1977, 2015, 20…
## $ duration            &lt;dbl&gt; 178, 194, 124, 173, 152, 136, 125, 141, 164, 93, 1…
## $ gross               &lt;dbl&gt; 7.61e+08, 6.59e+08, 6.52e+08, 6.23e+08, 5.33e+08, …
## $ budget              &lt;dbl&gt; 2.37e+08, 2.00e+08, 1.50e+08, 2.20e+08, 1.85e+08, …
## $ cast_facebook_likes &lt;dbl&gt; 4834, 45223, 8458, 87697, 57802, 37723, 13485, 920…
## $ votes               &lt;dbl&gt; 886204, 793059, 418214, 995415, 1676169, 534658, 9…
## $ reviews             &lt;dbl&gt; 3777, 2843, 1934, 2425, 5312, 3917, 1752, 1752, 35…
## $ rating              &lt;dbl&gt; 7.9, 7.7, 7.0, 8.1, 9.0, 6.5, 8.7, 7.5, 8.5, 7.2, …</code></pre>
<pre class="r"><code># First glance; data types for each variable seem appropriate

# Check for missing values
movies_nomissing &lt;- movies %&gt;% 
  drop_na()
# We see that after dropping all observations with at least one missing value, the number of observations remain the same.

# Check for missing values (alternate/clearer method)
movies %&gt;%
  summarise_all(~sum(is.na(.))) %&gt;% 
  gather() %&gt;% 
  arrange(desc(value))</code></pre>
<pre><code>## # A tibble: 11 × 2
##    key                 value
##    &lt;chr&gt;               &lt;int&gt;
##  1 title                   0
##  2 genre                   0
##  3 director                0
##  4 year                    0
##  5 duration                0
##  6 gross                   0
##  7 budget                  0
##  8 cast_facebook_likes     0
##  9 votes                   0
## 10 reviews                 0
## 11 rating                  0</code></pre>
<pre class="r"><code># Alternatively, We see that there are no missing values for any variables in the dataset.

# Identified all duplicates (please read the disclaimer in our comment below)
movies %&gt;%
  group_by(title) %&gt;% 
  filter(n()&gt;1) %&gt;%
  summarise(count(title))</code></pre>
<pre><code>## # A tibble: 53 × 2
##    title                       `count(title)`
##    &lt;chr&gt;                                &lt;int&gt;
##  1 A Nightmare on Elm Street                2
##  2 Across the Universe                      2
##  3 Alice in Wonderland                      2
##  4 Aloha                                    2
##  5 Around the World in 80 Days              2
##  6 Brothers                                 2
##  7 Carrie                                   2
##  8 Chasing Liberty                          2
##  9 Cinderella                               2
## 10 Clash of the Titans                      2
## # … with 43 more rows</code></pre>
<pre class="r"><code># We observe that 53 movie tites had duplicates - 52 movie titles had one duplicate while the title &quot;Home&quot; had 2 duplicates. A disclaimer here: we note that for these duplicates, the &quot;Voting&quot; variable had different values. However, we do not use &quot;Voting&quot; in any further part of our exercise and hence decided that it would be more prudent to remove these duplicates since the values of other key variables for each of these movie titles were exactly the same.</code></pre>
<pre class="r"><code># Remove duplicates
cleaned_movies &lt;- movies %&gt;% 
  distinct(title, .keep_all = TRUE)
# Now, we clean the data and use this new dataframe henceforth.</code></pre>
<p>Henceforth, the cleaned_movies dataframe will be used.</p>
<ul>
<li>Produce a table with the count of movies by genre, ranked in descending order</li>
</ul>
<pre class="r"><code># Create a table with count of movies by genre, ranked in descending order
movie_by_genre &lt;- cleaned_movies %&gt;% 
  group_by(genre) %&gt;% 
  summarise(count = n()) %&gt;% 
  arrange(desc(count))

movie_by_genre</code></pre>
<pre><code>## # A tibble: 17 × 2
##    genre       count
##    &lt;chr&gt;       &lt;int&gt;
##  1 Comedy        844
##  2 Action        719
##  3 Drama         484
##  4 Adventure     281
##  5 Crime         198
##  6 Biography     135
##  7 Horror        128
##  8 Animation      35
##  9 Fantasy        26
## 10 Documentary    25
## 11 Mystery        15
## 12 Sci-Fi          7
## 13 Family          3
## 14 Musical         2
## 15 Romance         2
## 16 Western         2
## 17 Thriller        1</code></pre>
<pre class="r"><code># View newly created table</code></pre>
<ul>
<li>Produce a table with the average gross earning and budget (<code>gross</code> and <code>budget</code>) by genre. Calculate a variable <code>return_on_budget</code> which shows how many $ did a movie make at the box office for each $ of its budget. Ranked genres by this <code>return_on_budget</code> in descending order</li>
</ul>
<pre class="r"><code># Summarise the gross, budget and return_on_budget variables
mean_grossandbudget &lt;- cleaned_movies %&gt;% 
  group_by(genre) %&gt;% 
  summarise(mean_gross = mean(gross), mean_budget = mean(budget)) %&gt;%
  mutate(return_on_budget = mean_gross / mean_budget) %&gt;% 
  arrange(desc(return_on_budget))

# View newly created table
mean_grossandbudget</code></pre>
<pre><code>## # A tibble: 17 × 4
##    genre       mean_gross mean_budget return_on_budget
##    &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;
##  1 Musical      92084000     3189500          28.9    
##  2 Family      149160478.   14833333.         10.1    
##  3 Western      20821884     3465000           6.01   
##  4 Documentary  17353973.    5887852.          2.95   
##  5 Horror       37782310.   13804379.          2.74   
##  6 Fantasy      41902674.   18484615.          2.27   
##  7 Comedy       42487808.   24458506.          1.74   
##  8 Mystery      69117136.   41500000           1.67   
##  9 Animation    98433792.   61701429.          1.60   
## 10 Biography    45201805.   28543696.          1.58   
## 11 Adventure    94350236.   64692313.          1.46   
## 12 Drama        36754959.   25832605.          1.42   
## 13 Crime        37601525.   26527405.          1.42   
## 14 Romance      31264848.   25107500           1.25   
## 15 Action       86270343.   70774558.          1.22   
## 16 Sci-Fi       29788371.   27607143.          1.08   
## 17 Thriller         2468      300000           0.00823</code></pre>
<ul>
<li>Produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. Don’t just show the total gross amount, but also the mean, median, and standard deviation per director.</li>
</ul>
<pre class="r"><code># Summarise the total gross amount, the mean, median, and standard deviation for the top 15 directors by total gross amount
director_analysis &lt;- cleaned_movies %&gt;% 
  group_by(director) %&gt;%
  summarise(total_gross = sum(gross),
            mean = mean(gross),
            median = median(gross),
            sd = sd(gross)) %&gt;% 
  arrange(desc(total_gross)) %&gt;% 
  top_n(15, total_gross)

# View newly created table
director_analysis</code></pre>
<pre><code>## # A tibble: 15 × 5
##    director          total_gross       mean     median         sd
##    &lt;chr&gt;                   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
##  1 Steven Spielberg   4014061704 174524422. 164435221  101421051.
##  2 Michael Bay        2195443511 182953626. 168468240. 125789167.
##  3 James Cameron      1909725910 318287652. 175562880. 309171337.
##  4 Christopher Nolan  1813227576 226653447  196667606. 187224133.
##  5 George Lucas       1741418480 348283696  380262555  146193880.
##  6 Robert Zemeckis    1619309108 124562239. 100853835   91300279.
##  7 Tim Burton         1557078534 111219895.  69791834   99304293.
##  8 Sam Raimi          1443167519 180395940. 138480208  174705230.
##  9 Clint Eastwood     1378321100  72543216.  46700000   75487408.
## 10 Francis Lawrence   1358501971 271700394. 281666058  135437020.
## 11 Ron Howard         1335988092 111332341  101587923   81933761.
## 12 Gore Verbinski     1329600995 189942999. 123207194  154473822.
## 13 Andrew Adamson     1137446920 284361730  279680930. 120895765.
## 14 Shawn Levy         1129750988 102704635.  85463309   65484773.
## 15 Ridley Scott       1128857598  80632686.  47775715   68812285.</code></pre>
<ul>
<li>Finally, ratings. Produce a table that describes how ratings are distributed by genre. We don’t want just the mean, but also, min, max, median, SD and some kind of a histogram or density graph that visually shows how ratings are distributed.</li>
</ul>
<pre class="r"><code># Summarise the mean, min, max, median and SD of ratings for each genre of movies
ratings_by_genre &lt;- cleaned_movies %&gt;% 
  group_by(genre) %&gt;% 
  summarise(mean = mean(rating),
            min = min(rating),
            max = max(rating),
            median = median(rating),
            sd = sd(rating))

ratings_by_genre</code></pre>
<pre><code>## # A tibble: 17 × 6
##    genre        mean   min   max median     sd
##    &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1 Action       6.23   2.1   9     6.3   1.04 
##  2 Adventure    6.51   2.3   8.6   6.6   1.11 
##  3 Animation    6.65   4.5   8     6.9   0.968
##  4 Biography    7.11   4.5   8.9   7.2   0.760
##  5 Comedy       6.11   1.9   8.8   6.2   1.02 
##  6 Crime        6.92   4.8   9.3   6.9   0.853
##  7 Documentary  6.66   1.6   8.5   7.4   1.77 
##  8 Drama        6.74   2.1   8.8   6.8   0.915
##  9 Family       6.5    5.7   7.9   5.9   1.22 
## 10 Fantasy      6.08   4.3   7.9   6.2   0.953
## 11 Horror       5.79   3.6   8.5   5.85  0.987
## 12 Musical      6.75   6.3   7.2   6.75  0.636
## 13 Mystery      6.84   4.6   8.5   6.7   0.910
## 14 Romance      6.65   6.2   7.1   6.65  0.636
## 15 Sci-Fi       6.66   5     8.2   6.4   1.09 
## 16 Thriller     4.8    4.8   4.8   4.8  NA    
## 17 Western      5.7    4.1   7.3   5.7   2.26</code></pre>
<pre class="r"><code># View newly created table

ggplot(cleaned_movies, aes(rating)) +
  geom_density() +
  facet_wrap(~ genre, scales = &quot;free&quot;) +
  labs(title = &quot;Faceted density plots of movie ratings&quot;, x = &quot;Ratings&quot;, y = &quot;Density&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/ratings_by_genre-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Faceted density plots are not as effective or accurate because we earlier saw that there a select number of the genres (namely Sci-Fi, Family, Musical, Romance Western, Thriller) with especially few observations in them. The density function for those genres will thus not mean much.
# Notice also that the &quot;Thriller&quot; genre does not reflect any density plot because there was only one &quot;Thriller&quot; type movie within the entire dataset.

ggplot(cleaned_movies, aes(rating)) +
  geom_histogram() +
  facet_wrap(~ genre, scales = &quot;free&quot;) +
  labs(title = &quot;Faceted histogram plots of movie ratings&quot;, x = &quot;Ratings&quot;, y = &quot;Count&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/ratings_by_genre-2.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># The histogram plot is able to reflect much of the distribution.

ggplot(cleaned_movies, aes(rating)) + 
    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   colour=&quot;black&quot;, fill=&quot;white&quot;) +
    geom_density(alpha=.2, fill=&quot;#FF6666&quot;) +  # Overlay with transparent density plot
    facet_wrap(~genre, scales = &quot;free&quot;) +
    labs(title = &quot;Faceted density plots overlaid on histograms of movie ratings&quot;, x = &quot;Ratings&quot;, y = &quot;Density&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/ratings_by_genre-3.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># To view an overlay of a density plot on histogram (done just out of interest)</code></pre>
<p><u><strong><em>EDITOR’S NOTE:</em></strong></u></p>
<p>The faceted density plots are not as effective or accurate because we earlier saw that there is a select number of the genres (namely Sci-Fi, Family, Musical, Romance Western, Thriller) with especially few observations in them. The density function for those genres will thus not mean much. Also, we noticed that the “Thriller” genre does not reflect any density plot because there was only one “Thriller” type movie within the entire dataset. The histogram plots, on the other hand, are able to showcase the distributions.</p>
</div>
<div id="use-ggplot-to-answer-the-following" class="section level2">
<h2>Use <code>ggplot</code> to answer the following</h2>
<ul>
<li>Examine the relationship between <code>gross</code> and <code>cast_facebook_likes</code>. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes?</li>
</ul>
<pre class="r"><code># Plot total gross on number of facebook likes
ggplot(cleaned_movies, aes(cast_facebook_likes, gross, colour = genre)) +
  geom_point() +
  geom_smooth(se = FALSE)+
  xlim(0, 200000) + # Limit the x-scale to make the interpretation simpler
  scale_x_continuous(labels = ~ format(.x, scientific = FALSE)) +
  scale_y_continuous(labels = ~ format(.x, scientific = FALSE)) +
  labs(title = &quot;Scatterplot of total gross amount on number of facebook likes&quot;, x = &quot;Number of Facebook Likes&quot;, y = &quot;Gross&quot;) +
  theme(legend.key.size = unit(0.5, &#39;cm&#39;), #change legend key size
        legend.key.height = unit(0.5, &#39;cm&#39;), #change legend key height
        legend.key.width = unit(0.5, &#39;cm&#39;), #change legend key width
        legend.title = element_text(size=10), #change legend title font size
        legend.text = element_text(size=8))</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/gross_on_fblikes-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># As a sanity check, regress total gross on number of facebook likes
model &lt;- lm(gross ~ cast_facebook_likes, data = cleaned_movies)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = gross ~ cast_facebook_likes, data = cleaned_movies)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -4.45e+08 -4.33e+07 -2.22e+07  1.73e+07  7.08e+08 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         4.86e+07   1.53e+06    31.8   &lt;2e-16 ***
## cast_facebook_likes 7.31e+02   6.39e+01    11.4   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 70700000 on 2905 degrees of freedom
## Multiple R-squared:  0.0432, Adjusted R-squared:  0.0428 
## F-statistic:  131 on 1 and 2905 DF,  p-value: &lt;2e-16</code></pre>
<p>Although there is a positive correlation between the two variables, we believe that the facebook likes are not likely to be a good predictor of the money the movie will make at the box office based on the variability of outcomes as seen in the scatter plot. To confirm our intuition we did a basic regression of the likes on the gross revenues. We realize that there is a positive correlation and the predictor is significant, but based on the small r-squared value of around 0.05 we conclude that the predictive power of the facebook likes is not high.</p>
<ul>
<li>Examine the relationship between <code>gross</code> and <code>budget</code>. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.</li>
</ul>
<pre class="r"><code># Plot scatterplot of total gross on budget
ggplot(cleaned_movies, aes(budget, gross, colour = genre)) +
  geom_point(aes(alpha = 0.1)) +
  geom_smooth(se = FALSE) +
  scale_x_continuous(labels = ~ format(.x, scientific = FALSE)) +
  labs(title = &quot;Scatterplot of total gross amount on budget&quot;, x = &quot;Budget&quot;, y = &quot;Gross&quot;) +
  theme(legend.key.size = unit(0.5, &#39;cm&#39;), #change legend key size
        legend.key.height = unit(0.5, &#39;cm&#39;), #change legend key height
        legend.key.width = unit(0.5, &#39;cm&#39;), #change legend key width
        legend.title = element_text(size=12), #change legend title font size
        legend.text = element_text(size=10))</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/gross_on_budget-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># As a sanity check, regress total gross on budget
model &lt;- lm(gross ~ budget, data = movies)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = gross ~ budget, data = movies)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -2.22e+08 -2.60e+07 -1.24e+07  1.31e+07  4.94e+08 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 1.49e+07   1.40e+06    10.7   &lt;2e-16 ***
## budget      1.06e+00   2.34e-02    45.4   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 55600000 on 2959 degrees of freedom
## Multiple R-squared:  0.411,  Adjusted R-squared:  0.41 
## F-statistic: 2.06e+03 on 1 and 2959 DF,  p-value: &lt;2e-16</code></pre>
<p>The scatter plot displays a clear trend of higher budgets for movies being linked with higher gross revenues on box office. A simple linear regression confirms this positive relation with an r-squared value of 0.41 and a highly significant budget coefficient.</p>
<ul>
<li>Examine the relationship between <code>gross</code> and <code>rating</code>. Produce a scatterplot, faceted by <code>genre</code> and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?</li>
</ul>
<pre class="r"><code># Plot total gross on ratings
ggplot(cleaned_movies, aes(rating, gross, colour = factor(genre))) +
  geom_point(aes(alpha = 0.1)) +
  scale_x_continuous(labels = ~ format(.x, scientific = FALSE)) +
  ylim(0, 200000000) + # For clearer interpretation, we limit the y-scale
  facet_wrap(~ genre) +
  labs(title = &quot;Scatterplot of total gross amount on rating&quot;, x = &quot;Rating&quot;, y = &quot;Gross&quot;) +
  theme(legend.key.size = unit(0.5, &#39;cm&#39;), #change legend key size
        legend.key.height = unit(0.5, &#39;cm&#39;), #change legend key height
        legend.key.width = unit(0.5, &#39;cm&#39;), #change legend key width
        legend.title = element_text(size=12), #change legend title font size
        legend.text = element_text(size=10))</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/gross_on_rating-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># As a sanity check, regress total gross on ratings
model &lt;- lm(gross ~ rating, data = movies)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = gross ~ rating, data = movies)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -98974277 -42915526 -17134877  19432831 674365127 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -60537478    7899673   -7.66  2.4e-14 ***
## rating       18566860    1219996   15.22  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 69800000 on 2959 degrees of freedom
## Multiple R-squared:  0.0726, Adjusted R-squared:  0.0723 
## F-statistic:  232 on 1 and 2959 DF,  p-value: &lt;2e-16</code></pre>
<p>When plotting the rating against the gross revenues for the movies, we can clearly identify a positive relation. However, there is a strong spread for given ratings. This is confirmed by our simple least squared regression, yielding an r-squared value 0.07. When braking it down by genre we noticed for genres such as action there a lot of movies and whilst better ratings seem to be reflected in higher gross revenues, there still remain big discrepancies for given ratings (e.g. for a similar rating, the revenue may differ 10-fold). For other genres, such as documentaries, there seems to be no positive correlation between ratings and gross revenues. Though, this might also be down to a small sample size.</p>
</div>
</div>
<div id="returns-of-financial-stocks" class="section level1">
<h1>Returns of financial stocks</h1>
<blockquote>
<p>You may find useful the material on <a href="https://mam2023.netlify.app/reference/finance_data/">finance data sources</a>.</p>
</blockquote>
<p>We will use the <code>tidyquant</code> package to download historical data of stock prices, calculate returns, and examine the distribution of returns.</p>
<p>We must first identify which stocks we want to download data for, and for this we must know their ticker symbol; Apple is known as AAPL, Microsoft as MSFT, McDonald’s as MCD, etc. The file <code>nyse.csv</code> contains 508 stocks listed on the NYSE, their ticker <code>symbol</code>, <code>name</code>, the IPO (Initial Public Offering) year, and the sector and industry the company is in.</p>
<pre class="r"><code>nyse &lt;- read_csv(here::here(&quot;data&quot;,&quot;nyse.csv&quot;))</code></pre>
<p>Based on this dataset, create a table and a bar plot that shows the number of companies per sector, in descending order</p>
<pre class="r"><code>glimpse(nyse)</code></pre>
<pre><code>## Rows: 508
## Columns: 6
## $ symbol        &lt;chr&gt; &quot;MMM&quot;, &quot;ABB&quot;, &quot;ABT&quot;, &quot;ABBV&quot;, &quot;ACN&quot;, &quot;AAP&quot;, &quot;AFL&quot;, &quot;A&quot;, &quot;…
## $ name          &lt;chr&gt; &quot;3M Company&quot;, &quot;ABB Ltd&quot;, &quot;Abbott Laboratories&quot;, &quot;AbbVie …
## $ ipo_year      &lt;chr&gt; &quot;n/a&quot;, &quot;n/a&quot;, &quot;n/a&quot;, &quot;2012&quot;, &quot;2001&quot;, &quot;n/a&quot;, &quot;n/a&quot;, &quot;1999…
## $ sector        &lt;chr&gt; &quot;Health Care&quot;, &quot;Consumer Durables&quot;, &quot;Health Care&quot;, &quot;Heal…
## $ industry      &lt;chr&gt; &quot;Medical/Dental Instruments&quot;, &quot;Electrical Products&quot;, &quot;Ma…
## $ summary_quote &lt;chr&gt; &quot;https://www.nasdaq.com/symbol/mmm&quot;, &quot;https://www.nasdaq…</code></pre>
<pre class="r"><code># Summarise count of companies per sector
condensed &lt;- nyse %&gt;%
  group_by(sector) %&gt;% 
  summarize(count_company = n()) %&gt;% 
  arrange(desc(count_company))

# View table
condensed</code></pre>
<pre><code>## # A tibble: 12 × 2
##    sector                count_company
##    &lt;chr&gt;                         &lt;int&gt;
##  1 Finance                          97
##  2 Consumer Services                79
##  3 Public Utilities                 60
##  4 Capital Goods                    45
##  5 Health Care                      45
##  6 Energy                           42
##  7 Technology                       40
##  8 Basic Industries                 39
##  9 Consumer Non-Durables            31
## 10 Miscellaneous                    12
## 11 Transportation                   10
## 12 Consumer Durables                 8</code></pre>
<pre class="r"><code># Plot count of companies per sector
ggplot(data = condensed, mapping = aes(x = count_company, y = fct_reorder(sector, count_company))) +
  geom_col() +
  labs(title = &quot;Number of companies per sector in the NYSE&quot;, x = &quot;number of companies&quot;,y = &quot;sector&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/companies_per_sector-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>Next, let’s choose some stocks and their ticker symbols and download some data. You <strong>MUST</strong> choose 6 different stocks from the ones listed below; You should, however, add <code>SPY</code> which is the SP500 ETF (Exchange Traded Fund).</p>
<pre class="r"><code># Notice the cache=TRUE argument inthe chunk options. Because getting data is time consuming, 
# cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks &lt;- c(&quot;ACN&quot;,&quot;BLK&quot;,&quot;CPT&quot;,&quot;GM&quot;,&quot;HLT&quot;,&quot;LVS&quot;,&quot;SPY&quot; ) %&gt;%
  tq_get(get  = &quot;stock.prices&quot;,
         from = &quot;2011-01-01&quot;,
         to   = &quot;2022-08-31&quot;) %&gt;%
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame</code></pre>
<pre><code>## Rows: 19,804
## Columns: 8
## Groups: symbol [7]
## $ symbol   &lt;chr&gt; &quot;ACN&quot;, &quot;ACN&quot;, &quot;ACN&quot;, &quot;ACN&quot;, &quot;ACN&quot;, &quot;ACN&quot;, &quot;ACN&quot;, &quot;ACN&quot;, &quot;ACN&quot;…
## $ date     &lt;date&gt; 2011-01-03, 2011-01-04, 2011-01-05, 2011-01-06, 2011-01-07, …
## $ open     &lt;dbl&gt; 48.7, 48.8, 48.1, 48.2, 48.4, 48.2, 48.2, 49.0, 49.5, 49.6, 4…
## $ high     &lt;dbl&gt; 49.4, 48.9, 48.5, 48.6, 48.7, 48.7, 48.8, 49.9, 49.9, 50.0, 5…
## $ low      &lt;dbl&gt; 48.5, 48.1, 47.7, 47.9, 48.0, 48.1, 48.1, 49.0, 49.2, 49.5, 4…
## $ close    &lt;dbl&gt; 48.6, 48.3, 48.3, 48.5, 48.5, 48.1, 48.8, 49.9, 49.9, 50.0, 5…
## $ volume   &lt;dbl&gt; 2508700, 3169400, 3041600, 3859000, 2914600, 3177900, 3696100…
## $ adjusted &lt;dbl&gt; 39.0, 38.7, 38.7, 38.9, 38.9, 38.6, 39.1, 40.0, 40.0, 40.1, 4…</code></pre>
<p>Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.</p>
<pre class="r"><code>#calculate daily returns
myStocks_returns_daily &lt;- myStocks %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;daily&quot;, 
               type       = &quot;log&quot;,
               col_rename = &quot;daily_returns&quot;,
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly &lt;- myStocks %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;monthly&quot;, 
               type       = &quot;arithmetic&quot;,
               col_rename = &quot;monthly_returns&quot;,
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual &lt;- myStocks %&gt;%
  group_by(symbol) %&gt;%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = &quot;yearly&quot;, 
               type       = &quot;arithmetic&quot;,
               col_rename = &quot;yearly_returns&quot;,
               cols = c(nested.col))</code></pre>
<p>Create a table where you summarise monthly returns for each of the stocks and <code>SPY</code>; min, max, median, mean, SD.</p>
<pre class="r"><code># Summarise min, max, median, mean, sd of chosen stocks
monthly_return_summary &lt;- myStocks_returns_monthly %&gt;%
  group_by(symbol) %&gt;% 
  summarise(min = min(monthly_returns),
            max = max(monthly_returns),
            median = median(monthly_returns),
            mean = mean(monthly_returns),
            SD = sd(monthly_returns))

# View summary
monthly_return_summary</code></pre>
<pre><code>## # A tibble: 7 × 6
##   symbol    min   max  median    mean     SD
##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 ACN    -0.145 0.160 0.0216  0.0164  0.0613
## 2 BLK    -0.183 0.183 0.0211  0.0134  0.0663
## 3 CPT    -0.245 0.135 0.0154  0.0110  0.0561
## 4 GM     -0.310 0.281 0.00311 0.00635 0.0911
## 5 HLT    -0.298 0.220 0.0234  0.0137  0.0786
## 6 LVS    -0.257 0.304 0.00387 0.00622 0.0993
## 7 SPY    -0.125 0.127 0.0146  0.0106  0.0404</code></pre>
<p>Plot a density plot, using <code>geom_density()</code>, for each of the stocks</p>
<pre class="r"><code># Plot faceted monthly returns density plots for each stock
ggplot(myStocks_returns_monthly, aes(x = monthly_returns)) +
  geom_density() +
  facet_wrap(~ symbol) +
  labs(title = &quot;Density Plots&quot;,
       subtitle = &quot;Density Plots on Monthly Returns for each stock&quot;,
       x = &quot;Monthly Returns&quot;,
       y = &quot;Density&quot;)+
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/density_monthly_returns-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>What can you infer from this plot? Which stock is the riskiest? The least risky?</p>
<blockquote>
<p>TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.</p>
</blockquote>
<p>The different stocks have various distributions among their monthly returns. The average monthly return of a stock tends to be close to zero in the long term (that is, the majority of the return is around 0, as shown in the graphs above). From our perspective, based on the table and the graphs above, LVS is the riskiest stock, and CPT is the least risky.</p>
<p>Finally, make a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use <code>ggrepel::geom_text_repel()</code> to label each stock</p>
<pre class="r"><code># Plot mean monthly returns on standard deviation (risk)
ggplot(monthly_return_summary, aes(x = SD, y = mean, label = symbol)) +
  geom_point() +
  ggrepel::geom_text_repel() +
  labs(title = &quot;Return vs. Risk&quot;,
       subtitle = str_wrap(&quot;The expected monthly return (mean) of a stock and its corresponding risk (standard deviation)&quot;, 60),
       x = &quot;Standard Deviation&quot;,
       y = &quot;Average Monthly Return&quot;) +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/risk_return_plot-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?</p>
<blockquote>
<p>TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.</p>
</blockquote>
<p>Generally speaking, the riskier a stock is, the higher the expected return it would have. This can be observed in the group of stocks SPY, CPT, BLK, and HLT, as shown above. However, we see that GM and LVS with high risks do not have a higher expected return; this does not conform to our general understanding of high risk, high returns.</p>
</div>
<div id="on-your-own-spotify" class="section level1">
<h1>On your own: Spotify</h1>
<p>Spotify have an API, an Application Programming Interface. APIs are ways for computer programs to talk to each other. So while we use Spotify app to look up songs and artists, computers use the Spotify API to talk to the spotify server. There is an R package that allows R to talk to this API: <a href="https://www.rcharlie.com/spotifyr/"><code>spotifyr</code></a>. One of your team members, need to sign up and get a <a href="https://developer.spotify.com/dashboard/">Spotify developer account</a> and then you can download data about one’s Spotify usage. A detailed article on how to go about it can be found here <a href="https://towardsdatascience.com/explore-your-activity-on-spotify-with-r-and-spotifyr-how-to-analyze-and-visualize-your-stream-dee41cb63526">Explore your activity on Spotify with R and <em>spotifyr</em></a></p>
<p>If you do not want to use the API, you can download a sample of over 32K songs by having a look at <a href="https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md" class="uri">https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md</a></p>
<pre class="r"><code>spotify_songs &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv&#39;)</code></pre>
<p>The data dictionary can be found below</p>
<table>
<colgroup>
<col width="23%" />
<col width="23%" />
<col width="53%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>variable</strong></th>
<th><strong>class</strong></th>
<th><strong>description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>track_id</td>
<td>character</td>
<td>Song unique ID</td>
</tr>
<tr class="even">
<td>track_name</td>
<td>character</td>
<td>Song Name</td>
</tr>
<tr class="odd">
<td>track_artist</td>
<td>character</td>
<td>Song Artist</td>
</tr>
<tr class="even">
<td>track_popularity</td>
<td>double</td>
<td>Song Popularity (0-100) where higher is better</td>
</tr>
<tr class="odd">
<td>track_album_id</td>
<td>character</td>
<td>Album unique ID</td>
</tr>
<tr class="even">
<td>track_album_name</td>
<td>character</td>
<td>Song album name</td>
</tr>
<tr class="odd">
<td>track_album_release_date</td>
<td>character</td>
<td>Date when album released</td>
</tr>
<tr class="even">
<td>playlist_name</td>
<td>character</td>
<td>Name of playlist</td>
</tr>
<tr class="odd">
<td>playlist_id</td>
<td>character</td>
<td>Playlist ID</td>
</tr>
<tr class="even">
<td>playlist_genre</td>
<td>character</td>
<td>Playlist genre</td>
</tr>
<tr class="odd">
<td>playlist_subgenre</td>
<td>character</td>
<td>Playlist subgenre</td>
</tr>
<tr class="even">
<td>danceability</td>
<td>double</td>
<td>Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.</td>
</tr>
<tr class="odd">
<td>energy</td>
<td>double</td>
<td>Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.</td>
</tr>
<tr class="even">
<td>key</td>
<td>double</td>
<td>The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.</td>
</tr>
<tr class="odd">
<td>loudness</td>
<td>double</td>
<td>The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.</td>
</tr>
<tr class="even">
<td>mode</td>
<td>double</td>
<td>Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.</td>
</tr>
<tr class="odd">
<td>speechiness</td>
<td>double</td>
<td>Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.</td>
</tr>
<tr class="even">
<td>acousticness</td>
<td>double</td>
<td>A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.</td>
</tr>
<tr class="odd">
<td>instrumentalness</td>
<td>double</td>
<td>Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.</td>
</tr>
<tr class="even">
<td>liveness</td>
<td>double</td>
<td>Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.</td>
</tr>
<tr class="odd">
<td>valence</td>
<td>double</td>
<td>A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).</td>
</tr>
<tr class="even">
<td>tempo</td>
<td>double</td>
<td>The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.</td>
</tr>
<tr class="odd">
<td>duration_ms</td>
<td>double</td>
<td>Duration of song in milliseconds</td>
</tr>
</tbody>
</table>
<p>In this dataset, there are only 6 types of <code>playlist_genre</code> , but we can still try to perform EDA on this dataset.</p>
<p>Produce a one-page summary describing this dataset. Here is a non-exhaustive list of questions:</p>
<ol style="list-style-type: decimal">
<li>What is the distribution of songs’ popularity (<code>track_popularity</code>). Does it look like a Normal distribution?</li>
</ol>
<p>The songs’ popularity score distribution resembles a normal distribution to a certain extent only; it looks somewhat symmetrical about the peak. However, there is one glaring exception; a significant number of songs are clustered on the lower end of the popularity score (score &lt;= 25), thus breaking the resemblance to a true normal distribution.</p>
<pre class="r"><code># Density plot
ggplot(spotify_songs, aes(track_popularity)) +
  geom_density() +
  labs(x = &quot;track popularity&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/distributions-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Histogram plot
ggplot(spotify_songs, aes(track_popularity)) +
  geom_histogram() +
  labs(x = &quot;track popularity&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/distributions-2.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># A clearer picture by super imposing density and histogram plots
ggplot(spotify_songs, aes(track_popularity)) +
  geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis
                   colour=&quot;black&quot;, fill=&quot;white&quot;) +
  geom_density() +
  labs(x = &quot;track popularity&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/distributions-3.png" width="648" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li>There are 12 <a href="https://developer.spotify.com/documentation/web-api/reference/object-model/#audio-features-object">audio features</a> for each track, including confidence measures like <code>acousticness</code>, <code>liveness</code>, <code>speechines</code>and <code>instrumentalness</code>, perceptual measures like <code>energy</code>, <code>loudness</code>, <code>danceability</code> and <code>valence</code> (positiveness), and descriptors like <code>duration</code>, <code>tempo</code>, <code>key</code>, and <code>mode</code>. How are they distributed? can you roughly guess which of these variables is closer to Normal just by looking at summary statistics?</li>
</ol>
<p>The “danceability”, “energy” and “duration_ms” variables have distributions that are closer to normal. The rest have extremely skewed or distorted distributions. The “mode” variable is a binary variable that takes on only values of 0 or 1.</p>
<pre class="r"><code># Create a long data table to represent each audio feature as a measurement score
audio_feature_list &lt;- spotify_songs %&gt;%
  gather(audio_feature, measurement, danceability:duration_ms, factor_key = TRUE)

# Create summary statistics and observe
summary_stats &lt;- audio_feature_list %&gt;%
  group_by(audio_feature) %&gt;% 
  summarise(mean = mean(measurement),
            median = median(measurement),
            min = min(measurement),
            max = max(measurement),
            sd = sd(measurement))

# View summary stats
summary_stats</code></pre>
<pre><code>## # A tibble: 12 × 6
##    audio_feature           mean   median         min        max        sd
##    &lt;fct&gt;                  &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;
##  1 danceability          0.655   6.72e-1    0             0.983     0.145
##  2 energy                0.699   7.21e-1    0.000175      1         0.181
##  3 key                   5.37    6   e+0    0            11         3.61 
##  4 loudness             -6.72   -6.17e+0  -46.4           1.27      2.99 
##  5 mode                  0.566   1   e+0    0             1         0.496
##  6 speechiness           0.107   6.25e-2    0             0.918     0.101
##  7 acousticness          0.175   8.04e-2    0             0.994     0.220
##  8 instrumentalness      0.0847  1.61e-5    0             0.994     0.224
##  9 liveness              0.190   1.27e-1    0             0.996     0.154
## 10 valence               0.511   5.12e-1    0             0.991     0.233
## 11 tempo               121.      1.22e+2    0           239.       26.9  
## 12 duration_ms      225800.      2.16e+5 4000        517810     59834.</code></pre>
<pre class="r"><code># Density plot
ggplot(audio_feature_list, aes(measurement)) +
  geom_density()+
  facet_wrap(~ audio_feature, scales = &quot;free&quot;) +
  labs(title = &quot;Audio Features&#39; density plot analysis&quot;, x = &quot;Measurement&quot;, y = &quot;Density&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/audio_feature_analysis-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Histogram plot
ggplot(audio_feature_list, aes(measurement)) +
  geom_histogram()+
  facet_wrap(~ audio_feature, scales = &quot;free&quot;) +
  labs(title = &quot;Audio Features&#39; histogram analysis&quot;, x = &quot;Measurement&quot;, y = &quot;Count&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/audio_feature_analysis-2.png" width="648" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li>How are <code>job_satisfaction</code> and <code>work_life_balance</code> distributed? Don’t just report counts, but express categories as % of total</li>
</ol>
<p>We are unsure if this question above belongs in this homework set since the variables “job_satisfaction” and “work_life_balance” are not found in the Spotify songs dataset.</p>
<ol style="list-style-type: decimal">
<li>Is there any relationship between <code>valence</code> and <code>track_popularity</code>? <code>danceability</code> and <code>track_popularity</code> ?</li>
</ol>
<p>The scatterplots of both track popularity on valence and track popularity on danceability reflect no clear relatioship; no clear line can be plotted through the points. An interpretation could be that every person’s music taste is really diverse and both valence and danceability are no significant indicators of whether a track will be a hit or miss amongst the crowd.</p>
<pre class="r"><code># Produce scatterplot of track popularity on valence
ggplot(spotify_songs, aes(valence, track_popularity)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  labs(title = &quot;Scatterplot of Track Popularity on Valence&quot;, x = &quot;Valence&quot;, y = &quot;Track Popularity&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/popularity_on_valence-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># As a sanity check, regress popularity on valence
model &lt;- lm(track_popularity ~ valence, data = spotify_songs)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = track_popularity ~ valence, data = spotify_songs)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -44.15 -18.49   2.87  19.50  57.51 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   40.659      0.332  122.55  &lt; 2e-16 ***
## valence        3.561      0.591    6.02  1.7e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 25 on 32831 degrees of freedom
## Multiple R-squared:  0.0011, Adjusted R-squared:  0.00107 
## F-statistic: 36.3 on 1 and 32831 DF,  p-value: 1.71e-09</code></pre>
<pre class="r"><code>ggplot(spotify_songs, aes(danceability, track_popularity)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  labs(title = &quot;Scatterplot of Track Popularity on Danceability&quot;, x = &quot;Danceability&quot;, y = &quot;Track Popularity&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/popularity_on_danceability-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Produce scatterplot of track popularity on danceability

# As a sanity check, regress popularity on valence
model &lt;- lm(track_popularity ~ danceability, data = spotify_songs)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = track_popularity ~ danceability, data = spotify_songs)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -46.02 -18.42   2.93  19.48  57.10 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    35.176      0.636    55.3   &lt;2e-16 ***
## danceability   11.150      0.948    11.8   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 24.9 on 32831 degrees of freedom
## Multiple R-squared:  0.00419,    Adjusted R-squared:  0.00416 
## F-statistic:  138 on 1 and 32831 DF,  p-value: &lt;2e-16</code></pre>
<ol style="list-style-type: decimal">
<li><code>mode</code> indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. Do songs written on a major scale have higher <code>danceability</code> compared to those in minor scale? What about <code>track_popularity</code>?</li>
</ol>
<p>From the scatterplot, there is no clear relationship between danceability and mode. We should expect to see a clustering in opposite diagonals of the scatterplot if there were a relationship between both variables; however, that’s not what we see for both the points along the mode = 1 line and the points along the mode = 0 line. Similarly, we do not see a clear relationship between track popularity and mode. Even going one step further to observe the relationship within each genre does not yield any change.</p>
<pre class="r"><code># Create scatterplot of mode on danceability
ggplot(spotify_songs, aes(danceability, mode)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  labs(title = &quot;Scatterplot of Mode on Danceability&quot;, x = &quot;Danceability&quot;, y = &quot;Mode&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/modality_on_danceability-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Create scatterplot of mode on danceability
ggplot(spotify_songs, aes(danceability, mode)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  facet_wrap(~ playlist_genre) +
  labs(title = &quot;Faceted scatterplots of Mode on Danceability&quot;, x = &quot;Danceability&quot;, y = &quot;Mode&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/modality_on_danceability-2.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Create scatterplot of mode on track popularity according to genre
ggplot(spotify_songs, aes(track_popularity, mode)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  labs(title = &quot;Scatterplot of Mode on Track Popularity&quot;, x = &quot;Track Popularity&quot;, y = &quot;Mode&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/modality_on_popularity-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Create scatterplot of track popularity on danceability for each of the 6 genres
ggplot(spotify_songs, aes(track_popularity, mode)) +
  geom_point(aes(alpha = 0.1), show.legend = FALSE) +
  facet_wrap(~ playlist_genre) +
  labs(title = &quot;Faceted scatterplots of Mode on Track Popularity&quot;, x = &quot;Track Popularity&quot;, y = &quot;Mode&quot;)</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/modality_on_popularity-2.png" width="648" style="display: block; margin: auto;" /></p>
<p>Narrative:</p>
<p>Overall, the songs’ popularity score distribution resemble a normal distribution to a certain extent only; it looks somewhat symmetrical about the peak. However, there is one glaring exception; a significant number of songs are clustered on the lower end of the popularity score (score &lt;= 25), thus breaking the resemblance to a true normal distribution. In terms of the different audio features, the “danceability”, “energy” and “duration_ms” variables have distributions that are closer to normal. The rest have extremely skewed or distorted distributions. The “mode” variable is a binary variable that takes on only values of 0 or 1.</p>
<p>Now, we move on to drawing relationships between key variables. The scatterplots of both track popularity on valence and track popularity on danceability reflect no clear relatioship; no clear line can be plotted through the points. An interpretation could be that every person’s music taste is really diverse and both valence and danceability are no significant indicators of whether a track will be a hit or miss amongst the crowd. From the scatterplots, there is also no clear relationship between danceability and mode. We should expect to see a clustering in opposite diagonals of the scatterplot if there were a relationship between both variables; however, that’s not what we see for both the points along the mode = 1 line and the points along the mode = 0 line. Similarly, we do not see a clear relationship between track popularity and mode. Even going one step further to observe the relationship within each genre does not yield any change.</p>
</div>
<div id="challenge-1-replicating-a-chart" class="section level1">
<h1>Challenge 1: Replicating a chart</h1>
<p>The purpose of this exercise is to reproduce a plot using your <code>dplyr</code> and <code>ggplot2</code> skills. It builds on exercise 1, the San Francisco rentals data.</p>
<p>You have to create a graph that calculates the cumulative % change for 0-, 1-1, and 2-bed flats between 2000 and 2018 for the top twelve cities in Bay Area, by number of ads that appeared in Craigslist. Your final graph should look like this</p>
<p><img src="images/challenge1.png" /></p>
<pre class="r"><code># Summarise popularity of cities in terms of percent of ads that appeared
# Find the top 12 cities (We reused code from the section above since they are linked)
top_12_cities &lt;- rent %&gt;% 
  group_by(city) %&gt;% 
  summarize(count_city = n()) %&gt;% 
  arrange(desc(count_city)) %&gt;% 
  mutate(frequency = count_city / sum(count_city)) %&gt;% 
  top_n(12, frequency)

# View top 12 cities
top_12_cities</code></pre>
<pre><code>## # A tibble: 12 × 3
##    city          count_city frequency
##    &lt;chr&gt;              &lt;int&gt;     &lt;dbl&gt;
##  1 san francisco      55730    0.278 
##  2 san jose           13733    0.0684
##  3 oakland             9443    0.0470
##  4 santa rosa          6230    0.0310
##  5 santa cruz          5464    0.0272
##  6 san mateo           5127    0.0255
##  7 sunnyvale           4526    0.0225
##  8 mountain view       4414    0.0220
##  9 berkeley            4201    0.0209
## 10 santa clara         4171    0.0208
## 11 palo alto           3916    0.0195
## 12 union city          3451    0.0172</code></pre>
<pre class="r"><code># Based on the cumulative_percent_change code/formula that you sent us via Slack, we focused on the cumulative median price change
cumulative_percent_change &lt;- rent %&gt;% 
  filter(city %in% top_12_cities$city &amp; beds &lt;= 2) %&gt;% 
  group_by(city, beds, year) %&gt;%
  summarise(median_price = median(price)) %&gt;%
  ungroup() %&gt;% 
  mutate(pct_change = (median_price/lag(median_price))) %&gt;% 
  mutate(pct_change = ifelse(is.na(pct_change), 1, pct_change)) %&gt;% 
  mutate(percent_change = cumprod(pct_change), percent_change = parse_number(scales::percent(percent_change)))

# Plot cumulative percent median price change across time for each of the unique bed-city combination  
ggplot(data = cumulative_percent_change, mapping = aes(year, percent_change, colour = factor(city))) +
    facet_grid(vars(beds), vars(city)) +
    geom_line() +
    labs(title = &quot;Cumulative Percentage Change for flats&quot;, x = &quot;Year&quot;, y = &quot;Cumulative Percent Change (%)&quot;, ) +
    theme(axis.title = element_text(size = 14),
          axis.text = element_text(size = 8),
          axis.text.x = element_text(angle = 90),
          strip.text = element_text(size = 5),
          strip.background = element_rect(fill=&quot;lightblue&quot;, colour=&quot;black&quot;, size=1),
          legend.key.size = unit(0.5, &#39;cm&#39;), #change legend key size
          legend.key.height = unit(0.5, &#39;cm&#39;), #change legend key height
          legend.key.width = unit(0.5, &#39;cm&#39;), #change legend key width
          legend.title = element_text(size=10), #change legend title font size
          legend.text = element_text(size=8),
          legend.position = &quot;bottom&quot;) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = &quot;Cities&quot;, breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ . , name = &quot;Beds&quot;, breaks = NULL, labels = NULL))</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/percent_change_top_12-1.png" width="648" style="display: block; margin: auto;" /></p>
</div>
<div id="challenge-2-2016-california-contributors-plots" class="section level1">
<h1>Challenge 2: 2016 California Contributors plots</h1>
<p>As discussed in class, I would like you to reproduce the plot that shows the top ten cities in highest amounts raised in political contributions in California during the 2016 US Presidential election.</p>
<p><img src="../../../images/challenge2.png" width="100%" style="display: block; margin: auto;" /></p>
<p>To get this plot, you must join two dataframes; the one you have with all contributions, and data that can translate zipcodes to cities. You can find a file with all US zipcodes, e.g., here <a href="http://www.uszipcodelist.com/download.html" class="uri">http://www.uszipcodelist.com/download.html</a>.</p>
<p>The easiest way would be to create two plots and then place one next to each other. For this, you will need the <code>patchwork</code> package. <a href="https://cran.r-project.org/web/packages/patchwork/index.html" class="uri">https://cran.r-project.org/web/packages/patchwork/index.html</a></p>
<p>While this is ok, what if one asked you to create the same plot for the top 10 candidates and not just the top two? The most challenging part is how to reorder within categories, and for this you will find Julia Silge’s post on <a href="https://juliasilge.com/blog/reorder-within/">REORDERING AND FACETTING FOR GGPLOT2</a> useful.</p>
<pre class="r"><code># Make sure you use vroom() as it is significantly faster than read.csv()
# Load datasets
CA_contributors_2016 &lt;- vroom::vroom(here::here(&quot;data&quot;,&quot;CA_contributors_2016.csv&quot;))

zipcodes &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;zip_code_database.csv&quot;))

# Glimpse datasets
glimpse(CA_contributors_2016)</code></pre>
<pre><code>## Rows: 1,292,843
## Columns: 4
## $ cand_nm           &lt;chr&gt; &quot;Clinton, Hillary Rodham&quot;, &quot;Clinton, Hillary Rodham&quot;…
## $ contb_receipt_amt &lt;dbl&gt; 50.0, 200.0, 5.0, 48.3, 40.0, 244.3, 35.0, 100.0, 25…
## $ zip               &lt;dbl&gt; 94939, 93428, 92337, 95334, 93011, 95826, 90278, 902…
## $ contb_date        &lt;date&gt; 2016-04-26, 2016-04-20, 2016-04-02, 2016-11-21, 201…</code></pre>
<pre class="r"><code>glimpse(zipcodes)</code></pre>
<pre><code>## Rows: 42,522
## Columns: 16
## $ zip                  &lt;chr&gt; &quot;00501&quot;, &quot;00544&quot;, &quot;00601&quot;, &quot;00602&quot;, &quot;00603&quot;, &quot;006…
## $ type                 &lt;chr&gt; &quot;UNIQUE&quot;, &quot;UNIQUE&quot;, &quot;STANDARD&quot;, &quot;STANDARD&quot;, &quot;STAN…
## $ primary_city         &lt;chr&gt; &quot;Holtsville&quot;, &quot;Holtsville&quot;, &quot;Adjuntas&quot;, &quot;Aguada&quot;,…
## $ acceptable_cities    &lt;chr&gt; NA, NA, NA, NA, &quot;Ramey&quot;, &quot;Ramey&quot;, NA, NA, NA, NA,…
## $ unacceptable_cities  &lt;chr&gt; &quot;I R S Service Center&quot;, &quot;Irs Service Center&quot;, &quot;Co…
## $ state                &lt;chr&gt; &quot;NY&quot;, &quot;NY&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;PR&quot;, &quot;…
## $ county               &lt;chr&gt; &quot;Suffolk County&quot;, &quot;Suffolk County&quot;, &quot;Adjuntas&quot;, N…
## $ timezone             &lt;chr&gt; &quot;America/New_York&quot;, &quot;America/New_York&quot;, &quot;America/…
## $ area_codes           &lt;dbl&gt; 631, 631, 787939, 787, 787, NA, NA, 787939, 787, …
## $ latitude             &lt;dbl&gt; 40.8, 40.8, 18.2, 18.4, 18.4, 18.4, 18.4, 18.2, 1…
## $ longitude            &lt;dbl&gt; -73.0, -73.0, -66.7, -67.2, -67.2, -67.2, -67.2, …
## $ world_region         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…
## $ country              &lt;chr&gt; &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;US&quot;, &quot;…
## $ decommissioned       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ estimated_population &lt;dbl&gt; 384, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ notes                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;no NWS data,…</code></pre>
<pre class="r"><code># Join datasets (note that zip types are different across both datasets)
# Therefore we change them to be the same so that the datasets can be joined
CA_contributors_2016$zip&lt;-as.character(CA_contributors_2016$zip)
df &lt;- left_join(CA_contributors_2016, zipcodes, &quot;zip&quot;)

# Summarise total contributions raised by the top 10 candidates across every city
top_10_cand_overall &lt;- df %&gt;% 
  group_by(cand_nm) %&gt;%
  summarize(total = sum(contb_receipt_amt)) %&gt;%
  arrange(desc(total)) %&gt;% 
  top_n(10)

# Create long form data to incude only the top 10 candidates
long_form_top10_cand_all_cities &lt;- df %&gt;% 
  filter(cand_nm %in% top_10_cand_overall$cand_nm) %&gt;% 
  group_by(cand_nm, primary_city) %&gt;%
  summarise(total_contrib = sum(contb_receipt_amt))

# From previous dataset, create a table for the top 10 candidates across the top 10 cities
# Incorporate the use of reorder_within to reorder within a group
top_10_cities_per_cand &lt;- long_form_top10_cand_all_cities %&gt;% 
  group_by(cand_nm) %&gt;% 
  top_n(10, total_contrib) %&gt;% 
  ungroup() %&gt;%
  mutate(cand_nm = as.factor(cand_nm),
         primary_city = tidytext::reorder_within(primary_city, total_contrib, cand_nm)) 

# Plot faceted plots for each of the top 10 candidates across their respective top 10 cities
ggplot(top_10_cities_per_cand , aes(total_contrib, primary_city))+
  geom_col(aes(fill = cand_nm), show.legend = FALSE) +
  facet_wrap(~ cand_nm, ncol = 2, scales = &quot;free&quot;) +
  tidytext::scale_y_reordered() +
  labs(title = &quot;Comparisonn of contribution amounts raised&quot;, subtitle = &quot;In which cities did the top 10 candidates raise the most money?&quot;, x = &quot;Amount raised&quot;, y = &quot;City&quot;) +
  theme(axis.title = element_text(size = 14),
          axis.text = element_text(size = 8),
          strip.text = element_text(size = 5),
          strip.background = element_rect(fill=&quot;lightblue&quot;, colour=&quot;black&quot;, size=1))+
  scale_x_continuous(labels = ~ format(.x, scientific = FALSE),
                     sec.axis = sec_axis(~ . , name = &quot;Candidates&quot;, breaks = NULL, labels = NULL))</code></pre>
<p><img src="/english/blog/homework1_files/figure-html/load_CA_data-1.png" width="648" style="display: block; margin: auto;" /></p>
</div>
<div id="deliverables" class="section level1">
<h1>Deliverables</h1>
<p>There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown file as an HTML document (use the “Knit” button at the top of the script editor window) and upload it to Canvas.</p>
</div>
<div id="details" class="section level1">
<h1>Details</h1>
<ul>
<li><p>Who did you collaborate with: <u>Alex Scheuer, Dhruvi Mundra, Heng Jian Shun, Marta Wnek, Sharon Wenyu Xu, Xueying Liu</u></p></li>
<li><p>Approximately how much time did you spend on this problem set: <u>20 hours</u></p></li>
<li><p>What, if anything, gave you the most trouble: R-interface keeps giving problems. <u>Files keep running into errors when trying to knit them into html.</u></p></li>
</ul>
<p><strong>Please seek out help when you need it,</strong> and remember the <a href="https://mam2022.netlify.app/syllabus/#the-15-minute-rule" target="_blank">15-minute rule</a>. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack– and remember that I am here to help too!</p>
<blockquote>
<p>As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?</p>
</blockquote>
<p><u>Yes we are able to.</u></p>
</div>
<div id="rubric" class="section level1">
<h1>Rubric</h1>
<p>Check minus (1/5): Displays minimal effort. Doesn’t complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn’t use plots appropriate for the variables being analyzed.</p>
<p>Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output).</p>
<p>Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you’ve written additional text to describe how you interpret the output.</p>
</div>
